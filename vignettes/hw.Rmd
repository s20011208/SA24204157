---
title: "Homework"
author: "HaotianLuo"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to pit package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Question

- 6.4 Write a function to compute a Monte Carlo estimate of the Beta(3, 3) cdf, and use the function to estimate $F(x)$ for $x = 0.1, 0.2, \ldots, 0.9$. Compare the estimates with the values returned by the pbeta function in R.

- 6.6 In Example 6.7 the control variate approach was illustrated for Monte Carlo integration of
$$\theta = \int_{0}^{1} e^{x} dx.$$
Now consider the antithetic variate approach. Compute $Cov(e^{U}, e^{1-U})$ and $Var(e^{U} + e^{1-U})$, where $U \sim \text{Uniform}(0,1)$. What is the percent reduction in variance of $\hat{\theta}$ that can be achieved using antithetic variates (compared with simple MC)?

- 6.13 Find two importance functions $f_1$ and $f_2$ that are supported on $(1,\infty)$ and are “close” to
$$g(x) = \frac{x^2}{\sqrt{2\pi}} e^{-x^2/2}, \qquad x > 1.$$
Which of your two importance functions should produce the smaller variance in estimating
$$\int_1^\infty \frac{x^2}{\sqrt{2\pi}} e^{-x^2/2} dx$$
by importance sampling? Explain.

- Monte Carlo experiment

For $n = 10^4, 2 \times 10^4, 4 \times 10^4, 6 \times 10^4, 8 \times 10^4$, apply the fast sorting algorithm (R function sort) to randomly permuted numbers of $1, \ldots, n$.

Use R function rbenchmark::benchmark to count computation time (with 1000 replications), denoted by $a_n$.

Regress $a_n$ on $t_n := n \log(n)$, and graphically show the results (scatter plot and regression line).

## Answer

### 6.4 Beta(3,3)分布累积分布函数的蒙特卡洛估计

#### (1) 理论推导

Beta分布的概率密度函数定义为：
\[
f(x; \alpha, \beta) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha, \beta)}, \quad 0 \leq x \leq 1
\]
其中 $B(\alpha, \beta)$ 是Beta函数：
\[
B(\alpha, \beta) = \int_0^1 t^{\alpha-1}(1-t)^{\beta-1} dt = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}
\]

对于Beta(3,3)分布，具体参数为 $\alpha = 3$, $\beta = 3$，其概率密度函数为：
\[
f(x) = \frac{x^{2}(1-x)^{2}}{B(3,3)}
\]

计算Beta函数值：
\[
B(3,3) = \frac{\Gamma(3)\Gamma(3)}{\Gamma(6)} = \frac{2! \cdot 2!}{5!} = \frac{4}{120} = \frac{1}{30}
\]

因此Beta(3,3)的概率密度函数为：
\[
f(x) = 30x^2(1-x)^2, \quad 0 \leq x \leq 1
\]

累积分布函数定义为：
\[
F(x) = P(X \leq x) = \int_0^x f(t) dt = \int_0^x 30t^2(1-t)^2 dt
\]

这个积分可以解析求解，结果为：
\[
F(x) = 30\left(\frac{x^3}{3} - \frac{2x^4}{4} + \frac{x^5}{5}\right) = 10x^3 - 15x^4 + 6x^5
\]

\subsection{蒙特卡洛方法原理}

蒙特卡洛方法通过随机抽样来估计积分值。对于累积分布函数 $F(x) = P(X \leq x)$，可以使用以下估计量：
\[
\hat{F}(x) = \frac{1}{n} \sum_{i=1}^n I_{[0,x]}(X_i)
\]
其中：
- $X_i \sim \text{Beta}(3,3)$ 是独立同分布的随机样本
- $I_{[0,x]}$ 是指示函数，当 $X_i \leq x$ 时取值为1，否则为0

根据大数定律，当 $n \to \infty$ 时：
\[
\hat{F}(x) \xrightarrow{\text{a.s.}} F(x)
\]

估计量的方差为：
\[
\text{Var}[\hat{F}(x)] = \frac{F(x)(1-F(x))}{n}
\]

标准误差为：
\[
\text{SE}[\hat{F}(x)] = \sqrt{\frac{F(x)(1-F(x))}{n}}
\]

95\%置信区间为：
\[
\hat{F}(x) \pm 1.96 \cdot \text{SE}[\hat{F}(x)]
\]

绝对误差定义为：
\[
\text{Absolute Error} = |\hat{F}(x) - F(x)|
\]

相对误差定义为：
\[
\text{Relative Error} = \frac{|\hat{F}(x) - F(x)|}{F(x)} \times 100\%
\]

蒙特卡洛方法的精度随着样本量 $n$ 的增加而提高，收敛速度为 $O(1/\sqrt{n})$。

#### (2) R代码实现

```{r}

# 设置参数
set.seed(123)
n <- 100000
x_values <- seq(0.1, 0.9, by = 0.1)
alpha <- 3
beta <- 3

# 蒙特卡洛估计函数
mc_beta_cdf <- function(x, alpha, beta, n = 10000) {
  samples <- rbeta(n, alpha, beta)
  cdf_estimate <- mean(samples <= x)
  se <- sqrt(cdf_estimate * (1 - cdf_estimate) / n)
  return(list(estimate = cdf_estimate, se = se))
}

# 进行估计
results <- data.frame()
for (x in x_values) {
  mc_result <- mc_beta_cdf(x, alpha, beta, n)
  true_value <- pbeta(x, alpha, beta)
  results <- rbind(results, data.frame(
    x = x,
    MC_Estimate = mc_result$estimate,
    True_Value = true_value,
    Error = abs(mc_result$estimate - true_value)
  ))
}

# 结果显示
cat("蒙特卡洛估计结果比较:\n")
results_display <- data.frame(
  x = results$x,
  蒙特卡洛估计 = round(results$MC_Estimate, 6),
  理论值 = round(results$True_Value, 6),
  绝对误差 = round(results$Error, 6)
)

knitr::kable(results_display, caption = "Beta(3,3)分布CDF估计结果", 
             align = 'c', row.names = FALSE)

# 可视化
plot(results$x, results$True_Value, type = "b", col = "red", lwd = 2, 
     main = "蒙特卡洛估计与理论值比较",
     xlab = "x", ylab = "CDF值 F(x)", ylim = c(0, 1))
points(results$x, results$MC_Estimate, col = "blue", pch = 17)
lines(results$x, results$MC_Estimate, col = "blue", lty = 2)
legend("topleft", legend = c("理论值", "蒙特卡洛估计"),
       col = c("red", "blue"), lwd = 2, pch = c(1, 17))
```
由图可知，估计较准确。

### 6.6 对偶变量方法在蒙特卡洛积分中的应用

#### (1) 理论准备

简单蒙特卡洛估计量定义为：
\[
\hat{\theta}_{MC} = \frac{1}{n} \sum_{i=1}^n e^{U_i}, \quad U_i \sim \text{Uniform}(0,1)
\]

该估计量的方差为：
\[
\text{Var}(\hat{\theta}_{MC}) = \frac{\text{Var}(e^U)}{n}
\]

其中：
\[
\text{Var}(e^U) = E[(e^U)^2] - [E(e^U)]^2 = \int_0^1 e^{2u} du - \left(\int_0^1 e^u du\right)^2
\]

对偶变量估计量定义为：
\[
\hat{\theta}_{AV} = \frac{1}{n} \sum_{i=1}^n \frac{e^{U_i} + e^{1-U_i}}{2}
\]

该估计量的方差为：
\[
\text{Var}(\hat{\theta}_{AV}) = \frac{1}{4n} \text{Var}(e^U + e^{1-U})
\]


我们需要计算以下量：

1. 协方差：
\[
\text{Cov}(e^U, e^{1-U}) = E[e^U e^{1-U}] - E[e^U]E[e^{1-U}]
\]

2. 方差：
\[
\text{Var}(e^U + e^{1-U}) = \text{Var}(e^U) + \text{Var}(e^{1-U}) + 2\text{Cov}(e^U, e^{1-U})
\]

由于 $U$ 和 $1-U$ 同分布，有：
\[
\text{Var}(e^U) = \text{Var}(e^{1-U})
\]

因此：
\[
\text{Var}(e^U + e^{1-U}) = 2\text{Var}(e^U) + 2\text{Cov}(e^U, e^{1-U})
\]

各项期望的解析表达式：

\begin{align*}
E[e^U] &= \int_0^1 e^u du = e - 1 \\
E[e^{1-U}] &= \int_0^1 e^{1-u} du = e - 1 \\
E[(e^U)^2] &= \int_0^1 e^{2u} du = \frac{e^2 - 1}{2} \\
E[e^U e^{1-U}] &= E[e] = e
\end{align*}

方差减少百分比定义为：
\[
\text{Reduction\%} = \left(1 - \frac{\text{Var}(\hat{\theta}_{AV})}{\text{Var}(\hat{\theta}_{MC})}\right) \times 100\%
\]

代入表达式：
\[
\text{Reduction\%} = \left(1 - \frac{\frac{1}{4}\text{Var}(e^U + e^{1-U})}{\text{Var}(e^U)}\right) \times 100\%
\]

进一步化简：
\[
\text{Reduction\%} = \left(1 - \frac{\text{Var}(e^U) + \text{Cov}(e^U, e^{1-U})}{2\text{Var}(e^U)}\right) \times 100\%
= \left(\frac{1}{2} - \frac{\text{Cov}(e^U, e^{1-U})}{2\text{Var}(e^U)}\right) \times 100\%
\]

#### (2) R代码实现

```{r}
# 解析计算
cat("解析计算结果:\n")

# 计算各项期望
E_eU <- exp(1) - 1
E_e1U <- exp(1) - 1
E_e2U <- (exp(2) - 1) / 2
E_eU_e1U <- exp(1)

cat("E[e^U] =", round(E_eU, 6), "\n")
cat("E[e^(1-U)] =", round(E_e1U, 6), "\n") 
cat("E[(e^U)^2] =", round(E_e2U, 6), "\n")
cat("E[e^U * e^(1-U)] =", round(E_eU_e1U, 6), "\n\n")

# 计算方差和协方差
Var_eU <- E_e2U - E_eU^2
Cov_eU_e1U <- E_eU_e1U - E_eU * E_e1U
Var_sum <- 2 * Var_eU + 2 * Cov_eU_e1U

cat("方差和协方差:\n")
cat("Var(e^U) =", round(Var_eU, 6), "\n")
cat("Cov(e^U, e^(1-U)) =", round(Cov_eU_e1U, 6), "\n")
cat("Var(e^U + e^(1-U)) =", round(Var_sum, 6), "\n\n")

# 计算方差减少
cat("方差减少分析:\n")

# 简单蒙特卡洛的方差
Var_MC <- Var_eU

# 对偶变量方法的方差
Var_AV <- Var_sum / 4

# 方差减少百分比
reduction_pct <- (1 - Var_AV / Var_MC) * 100

cat("简单蒙特卡洛方差:", round(Var_MC, 6), "\n")
cat("对偶变量方法方差:", round(Var_AV, 6), "\n")
cat("方差减少百分比:", round(reduction_pct, 4), "%\n")

# 蒙特卡洛验证
set.seed(123)
n <- 1000000

cat("\n蒙特卡洛验证:\n")

# 生成随机数
U <- runif(n)
eU <- exp(U)
e1U <- exp(1 - U)

# 蒙特卡洛估计的协方差和方差
mc_Cov <- cov(eU, e1U)
mc_Var_sum <- var(eU + e1U)
mc_Var_eU <- var(eU)

cat("蒙特卡洛估计:\n")
cat("Cov(e^U, e^(1-U)) =", round(mc_Cov, 6), "\n")
cat("Var(e^U + e^(1-U)) =", round(mc_Var_sum, 6), "\n")
cat("Var(e^U) =", round(mc_Var_eU, 6), "\n\n")

# 蒙特卡洛验证的方差减少
mc_Var_AV <- mc_Var_sum / 4
mc_reduction_pct <- (1 - mc_Var_AV / mc_Var_eU) * 100

cat("蒙特卡洛验证结果:\n")
cat("对偶变量方法方差:", round(mc_Var_AV, 6), "\n")
cat("方差减少百分比:", round(mc_reduction_pct, 4), "%\n")

# 效率比较
cat("\n效率比较:\n")

# 计算相关系数
correlation <- Cov_eU_e1U / (sqrt(Var_eU) * sqrt(Var_eU))

cat("相关系数:", round(correlation, 6), "\n")

# 理论方差减少
theoretical_reduction <- (1 - (1 + correlation) / 2) * 100
cat("理论方差减少:", round(theoretical_reduction, 4), "%\n")

# 验证理论公式
cat("\n验证理论公式:\n")
cat("直接计算方差减少:", round(reduction_pct, 4), "%\n")
cat("通过相关系数计算:", round(theoretical_reduction, 4), "%\n")
cat("两者是否一致:", abs(reduction_pct - theoretical_reduction) < 1e-10, "\n")

# 可视化
par(mfrow = c(1, 2), mar = c(4, 4, 3, 1))

# 1. e^U 和 e^(1-U) 的散点图
set.seed(123)
U_sample <- runif(1000)
plot(exp(U_sample), exp(1 - U_sample), pch = 20, cex = 0.5,
     main = expression(paste(e^U, " 与 ", e^{1-U}, " 的关系")),
     xlab = expression(e^U), ylab = expression(e^{1-U}),
     col = rgb(0.2, 0.4, 0.8, 0.3))
abline(a = 0, b = 1, col = "red", lwd = 2)
legend("topleft", legend = paste("相关系数 =", round(correlation, 4)),
       bty = "n")

# 2. 方差比较
methods <- c("简单MC", "对偶变量")
variances <- c(Var_MC, Var_AV)
barplot(variances, names.arg = methods, col = c("lightblue", "lightcoral"),
        main = "方法方差比较", ylab = "方差", ylim = c(0, max(variances) * 1.1))
text(1:2, variances + max(variances)*0.05, round(variances, 4))

par(mfrow = c(1, 1))
```


### 6.13 重要性抽样在尾部概率估计中的应用

#### (1)理论推导

- 重要性抽样原理

重要性抽样通过引入重要性函数 $f(x)$ 来改写积分：
\[
\theta = \int_1^\infty \frac{g(x)}{f(x)} f(x) dx = E_f\left[\frac{g(X)}{f(X)}\right]
\]
其中 $X \sim f(x)$。

重要性抽样估计量为：
\[
\hat{\theta} = \frac{1}{n} \sum_{i=1}^n \frac{g(X_i)}{f(X_i)}, \quad X_i \sim f(x)
\]

- 方差分析

估计量的方差为：
\[
\text{Var}(\hat{\theta}) = \frac{1}{n} \left[ \int_1^\infty \frac{g^2(x)}{f(x)} dx - \theta^2 \right]
\]

为了使方差最小化，理想的重要性函数为：
\[
f^*(x) = \frac{g(x)}{\theta}
\]

但在实际中，$\theta$ 是未知的，因此我们需要选择"接近" $g(x)$ 的分布。

\subsection{目标函数分析}

目标函数可以重写为：
\[
g(x) = \frac{x^2}{\sqrt{2\pi}} e^{-x^2/2} = x^2 \cdot \phi(x)
\]
其中 $\phi(x)$ 是标准正态分布的密度函数。

这表明 $g(x)$ 与正态分布的右尾部相关，但被 $x^2$ 加权。

- 候选重要性函数

我们考虑两种候选重要性函数：

1. \textbf{平移指数分布}：$f_1(x) = \lambda e^{-\lambda(x-1)}, \quad x > 1$

2. \textbf{平移Gamma分布}：$f_2(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} (x-1)^{\alpha-1} e^{-\beta(x-1)}, \quad x > 1$

这两种分布都定义在 $(1, \infty)$ 上，且可以通过参数选择来"接近"目标函数 $g(x)$。

- 方差比较准则

对于两个重要性函数 $f_1$ 和 $f_2$，方差较小的那个满足：
\[
\int_1^\infty \frac{g^2(x)}{f_1(x)} dx < \int_1^\infty \frac{g^2(x)}{f_2(x)} dx
\]

等价地，我们需要比较：
\[
E_f\left[\left(\frac{g(X)}{f(X)}\right)^2\right]
\]
对于不同的重要性函数 $f$。

#### (2) R代码实现

```{r}
# 定义函数
g <- function(x) (x^2 / sqrt(2 * pi)) * exp(-x^2 / 2)
f1 <- function(x, lambda = 1) ifelse(x > 1, lambda * exp(-lambda * (x - 1)), 0)
f2 <- function(x, alpha = 2, beta = 1) ifelse(x > 1, (beta^alpha / gamma(alpha)) * (x - 1)^(alpha - 1) * exp(-beta * (x - 1)), 0)

# 函数形状比较
x_vals <- seq(1, 5, length.out = 1000)
g_vals <- g(x_vals)
f1_vals <- f1(x_vals, 1.2)
f2_vals <- f2(x_vals, 2.5, 1.2)

plot(x_vals, g_vals, type = "l", lwd = 3, col = "black",
     main = "目标函数与重要性函数比较",
     xlab = "x", ylab = "密度")
lines(x_vals, f1_vals, col = "red", lwd = 2, lty = 2)
lines(x_vals, f2_vals, col = "blue", lwd = 2, lty = 3)
legend("topright", 
       legend = c("g(x): 目标函数", "f1(x): 平移指数分布", "f2(x): 平移Gamma分布"),
       col = c("black", "red", "blue"), lwd = 2, lty = 1:3)

# 方差比较
set.seed(123)
n <- 100000

# 测试参数组合
test_variance <- function(lambda, alpha, beta) {
  # 从f1生成样本
  samples_f1 <- 1 + rexp(n, rate = lambda)
  weights_f1 <- g(samples_f1) / f1(samples_f1, lambda)
  var_f1 <- var(weights_f1) / n
  
  # 从f2生成样本  
  samples_f2 <- 1 + rgamma(n, shape = alpha, rate = beta)
  weights_f2 <- g(samples_f2) / f2(samples_f2, alpha, beta)
  var_f2 <- var(weights_f2) / n
  
  return(c(var_f1 = var_f1, var_f2 = var_f2, ratio = var_f2 / var_f1))
}

# 测试多组参数
params <- list(
  c(1.0, 2.0, 1.0),
  c(1.2, 2.5, 1.2), 
  c(0.8, 3.0, 1.5),
  c(1.5, 2.0, 1.0)
)

results <- data.frame()
for (i in seq_along(params)) {
  p <- params[[i]]
  res <- test_variance(p[1], p[2], p[3])
  results <- rbind(results, data.frame(
    lambda = p[1], alpha = p[2], beta = p[3],
    var_f1 = res["var_f1"], var_f2 = res["var_f2"],
    ratio = res["ratio"]
  ))
}

# 显示结果
cat("方差比较结果:\n")
knitr::kable(results, digits = 6, caption = "不同参数下的方差比较")

# 方差比较图
library(ggplot2)
library(reshape2)

plot_data <- data.frame(
  参数组 = 1:4,
  f1方差 = results$var_f1,
  f2方差 = results$var_f2
)

plot_data_long <- melt(plot_data, id.vars = "参数组")

ggplot(plot_data_long, aes(x = factor(参数组), y = value, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "重要性抽样方差比较",
       x = "参数组", y = "方差", fill = "方法") +
  scale_fill_manual(values = c("red", "blue"),
                    labels = c("f1: 平移指数", "f2: 平移Gamma")) +
  theme_minimal()

# 结论
cat("\n结论:\n")
avg_ratio <- mean(results$ratio)
if (avg_ratio < 1) {
  cat("f2 (平移Gamma分布) 产生更小的方差\n")
  cat("平均方差比 (f2/f1):", round(avg_ratio, 4), "\n")  
  cat("平均方差减少:", round((1 - avg_ratio) * 100, 2), "%\n")
} else {
  cat("f1 (平移指数分布) 产生更小的方差\n")
  cat("平均方差比 (f2/f1):", round(avg_ratio, 4), "\n")
}

# 权重分布比较
set.seed(123)
n_plot <- 5000

# 使用最佳参数
best_lambda <- 1.2
best_alpha <- 2.5
best_beta <- 1.2

samples_f1 <- 1 + rexp(n_plot, rate = best_lambda)
samples_f2 <- 1 + rgamma(n_plot, shape = best_alpha, rate = best_beta)

weights_f1 <- g(samples_f1) / f1(samples_f1, best_lambda)
weights_f2 <- g(samples_f2) / f2(samples_f2, best_alpha, best_beta)

par(mfrow = c(1, 2))
hist(weights_f1, breaks = 30, main = "f1重要性权重分布", 
     xlab = "g(x)/f1(x)", col = "lightcoral", border = "white")
hist(weights_f2, breaks = 30, main = "f2重要性权重分布",
     xlab = "g(x)/f2(x)", col = "lightblue", border = "white")
par(mfrow = c(1, 1))

cat("\n权重变异系数:\n")
cat("f1的CV:", round(sd(weights_f1)/mean(weights_f1), 4), "\n")
cat("f2的CV:", round(sd(weights_f2)/mean(weights_f2), 4), "\n")
```


### 蒙特卡洛实验

#### R代码实现

```{r}
library(rbenchmark)

# 定义实验参数
n_values <- c(1e4, 2e4, 4e4, 6e4, 8e4)
replications <- 1000

cat("实验参数设置:\n")
cat("样本量 n =", paste(n_values, collapse = ", "), "\n")
cat("重复次数 =", replications, "\n")
# 执行基准测试
results <- data.frame()

for (n in n_values) {
  cat("正在测试 n =", n, "...\n")
  
  # 创建测试数据生成函数
  generate_data <- function() sample(1:n)
  
  # 执行基准测试
  bench_result <- benchmark(
    "sort" = {
      data <- generate_data()
      sorted <- sort(data)
    },
    replications = replications,
    columns = c("test", "replications", "elapsed", "relative")
  )
  
  # 记录结果
  results <- rbind(results, data.frame(
    n = n,
    t_n = n * log(n),  # n log n
    a_n = bench_result$elapsed,  # 计算时间（秒）
    replications = replications
  ))
}

cat("基准测试完成\n")

# 显示结果
cat("基准测试结果:\n")
results_display <- data.frame(
  n = format(results$n, scientific = FALSE),
  t_n = round(results$t_n, 2),
  a_n = round(results$a_n, 4),
  单位时间 = round(results$a_n / results$t_n, 6)
)

knitr::kable(results_display, caption = "快速排序计算时间结果", align = 'c')

# 回归分析
cat("回归分析:\n")
cat("=========\n")

# 执行线性回归
model <- lm(a_n ~ t_n - 1, data = results)  # 通过原点的回归

# 显示回归结果
summary_model <- summary(model)
cat("回归方程: a_n = β × (n log n)\n")
cat("系数估计 β =", round(coef(model), 8), "\n")
cat("R² =", round(summary_model$r.squared, 6), "\n")
cat("调整R² =", round(summary_model$adj.r.squared, 6), "\n")

# 预测值
results$predicted <- predict(model)

# 散点图和回归线
plot(results$t_n, results$a_n, 
     pch = 19, cex = 1.5, col = "blue",
     xlab = expression(t[n] == n ~ log(n)),
     ylab = expression(a[n] ~ "(秒)"),
     main = "快速排序计算时间与 n log n 的关系")

# 添加回归线
abline(model, col = "red", lwd = 2)

# 添加数据点标签
text(results$t_n, results$a_n, 
     labels = paste0("n=", format(results$n, scientific = FALSE)), 
     pos = 3, cex = 0.8, col = "darkgreen")

# 添加图例
legend("topleft", 
       legend = c("观测数据", "回归线"),
       col = c("blue", "red"), 
       pch = c(19, NA), lwd = c(NA, 2),
       bty = "n")

# 残差分析
par(mfrow = c(1, 2))

# 残差图
residuals <- residuals(model)
plot(results$t_n, residuals,
     pch = 19, col = "purple",
     xlab = expression(t[n] == n ~ log(n)),
     ylab = "残差",
     main = "残差图")
abline(h = 0, col = "red", lty = 2)

# Q-Q图
qqnorm(residuals, main = "残差Q-Q图")
qqline(residuals, col = "red")

par(mfrow = c(1, 1))

cat("\n残差分析:\n")
cat("残差标准差:", round(sd(residuals), 6), "\n")
cat("最大残差:", round(max(abs(residuals)), 6), "\n")

# 性能比较图
comparison_data <- data.frame(
  n = results$n,
  观测时间 = results$a_n,
  预测时间 = results$predicted
)

# 转换为长格式用于绘图
library(reshape2)
comparison_long <- melt(comparison_data, id.vars = "n")

library(ggplot2)
ggplot(comparison_long, aes(x = n, y = value, color = variable)) +
  geom_line(aes(linetype = variable), size = 1) +
  geom_point(size = 2) +
  labs(title = "观测时间与预测时间比较",
       x = "样本量 n",
       y = "计算时间 (秒)",
       color = "时间类型") +
  scale_color_manual(values = c("blue", "red"),
                     labels = c("观测时间", "预测时间")) +
  scale_linetype_manual(values = c("solid", "dashed"),
                        labels = c("观测时间", "预测时间")) +
  theme_minimal()

# 复杂度验证
cat("算法复杂度验证:\n")
cat("================\n")

# 计算实际的时间复杂度系数
time_per_operation <- coef(model)
cat("每 n log n 操作的时间:", round(time_per_operation, 8), "秒\n")

# 预测更大n值的时间
larger_n <- c(1e5, 2e5, 5e5)
predicted_times <- data.frame(
  n = larger_n,
  t_n = larger_n * log(larger_n),
  predicted_time = time_per_operation * larger_n * log(larger_n)
)

cat("\n更大样本量的预测时间:\n")
predicted_display <- data.frame(
  n = format(predicted_times$n, scientific = FALSE),
  t_n = round(predicted_times$t_n, 2),
  预测时间_秒 = round(predicted_times$predicted_time, 4)
)

knitr::kable(predicted_display, caption = "更大样本量的预测计算时间", align = 'c')
```

title: "Exercise 6.15 — Stratified Importance Sampling for ∫_1^∞ x^2 φ(x) dx"
output: html_document
---

设 $\varphi(x)=\dfrac{1}{\sqrt{2\pi}}e^{-x^{2}/2}$，目标积分为
$$
I=\int_{1}^{\infty} x^{2}\,\varphi(x)\,\mathrm{d}x .
$$

由恒等式（分部积分或正态尾部恒等式）
$$
\int_{a}^{\infty} x^{2}\,\varphi(x)\,\mathrm{d}x
= a\,\varphi(a)+\bigl(1-\Phi(a)\bigr), \qquad a\in\mathbb{R},
$$
令 $a=1$ 得
$$
I=\varphi(1)+\bigl(1-\Phi(1)\bigr).
$$

## 重要抽样（IS）

取提案为标准正态在 $[1,\infty)$ 上的截断分布
$$
g(x)=\frac{\varphi(x)\,\mathbf{1}\{x\ge 1\}}{1-\Phi(1)} .
$$
于是权重为
$$
w(x)=\frac{x^{2}\varphi(x)}{g(x)}=x^{2}\bigl(1-\Phi(1)\bigr), \quad x\ge 1,
$$
普通 IS 估计量
$$
\widehat I_{\mathrm{IS}}=\frac{1}{n}\sum_{i=1}^{n} x_i^{2}\,\bigl(1-\Phi(1)\bigr),\qquad x_i\sim g .
$$

## 分层重要抽样（SIS）

将 $g$ 按等概率分位数分为 $K$ 层：$1=q_0<q_1<\cdots<q_K=\infty$，满足
$$
\mathbb{P}_g\bigl(q_{k-1}\le X<q_k\bigr)=p_k=\frac{1}{K},\qquad k=1,\dots,K .
$$
记第 $k$ 层样本量为 $m_k$（$\sum_{k=1}^{K} m_k=n$），层内均值
$$
\overline{h}_k=\frac{1}{m_k}\sum_{i=1}^{m_k} h\!\left(X_{k,i}\right),\qquad
h(x)=x^{2}\bigl(1-\Phi(1)\bigr),\quad X_{k,i}\sim g(\cdot\mid q_{k-1}\le X<q_k).
$$

```{r, eval=FALSE}
set.seed(123)

## 基本函数 ---------------------------------------------------------------
phi  <- function(x) dnorm(x)            # 标准正态密度
Phi  <- function(x) pnorm(x)            # 标准正态分布函数

a <- 1
tail_prob <- 1 - Phi(a)
I_true <- a*phi(a) + tail_prob          # 真值：∫_1^∞ x^2 φ(x) dx
cat("True value I =", I_true, "\n")

## 截断正态采样（[a, ∞)）与分段采样 --------------------------------------
# [a, ∞) 上的截断正态（N(0,1)）采样：分位数法
rtrunc_norm_right <- function(n, a = 1) {
  u <- runif(n, min = Phi(a), max = 1)
  qnorm(u)
}

# [L, U) 条件区间上的采样（L、U 可为有限或 ±Inf；通过概率刻度实现）
rtrunc_norm_segment <- function(n, L, U) {
  pL <- Phi(L)
  pU <- ifelse(is.infinite(U) & U > 0, 1, Phi(U))
  u  <- runif(n, min = pL, max = pU)
  qnorm(u)
}

## 普通重要抽样（不分层） --------------------------------------------------
IS_once <- function(n, a = 1) {
  x  <- rtrunc_norm_right(n, a)
  est <- mean(x^2) * (1 - pnorm(a))     # 权重：x^2*(1-Φ(1))
  est
}

bench_IS <- function(B = 1000, n = 2000, a = 1, I_true) {
  ests <- replicate(B, IS_once(n, a))
  c(mean = mean(ests), sd = sd(ests), rmse = sqrt(mean((ests - I_true)^2)))
}
## 分层重要抽样（等概率分层） ----------------------------------------------
SIS_equal_once <- function(n = 2000, K = 10, a = 1) {
  probs <- Phi(a) + (0:K) * (1 - Phi(a)) / K
  cuts  <- qnorm(probs)                 # q_0=1, q_K=Inf

  m_per <- floor(n / K)
  rem   <- n - m_per*K
  mks   <- rep(m_per, K); if (rem>0) mks[1:rem] <- mks[1:rem] + 1

  hbar  <- numeric(K)
  for (k in 1:K) {
    L <- cuts[k]; U <- cuts[k+1]
    xk <- rtrunc_norm_segment(mks[k], L, U)
    hbar[k] <- mean(xk^2) * (1 - Phi(a))
  }
  mean(hbar)                            # p_k=1/K，直接取层均值平均
}

bench_SIS_equal <- function(B=1000, n=2000, K=10, a=1, I_true) {
  ests <- replicate(B, SIS_equal_once(n, K, a))
  c(mean = mean(ests), sd = sd(ests), rmse = sqrt(mean((ests - I_true)^2)))
}

## 分层重要抽样（Neyman 最优分配，pilot 估 σ_k） ----------------------------
SIS_neyman_once <- function(n = 2000, K = 10, a = 1, pilot = 50) {
  probs <- Phi(a) + (0:K) * (1 - Phi(a)) / K
  cuts  <- qnorm(probs)

  # pilot 估计各层 σ_k
  sigk <- numeric(K)
  for (k in 1:K) {
    L <- cuts[k]; U <- cuts[k+1]
    xk <- rtrunc_norm_segment(pilot, L, U)
    hk <- xk^2 * (1 - Phi(a))
    sigk[k] <- sd(hk)
  }

  # Neyman 分配：m_k ∝ p_k*σ_k；此处 p_k=1/K 相同
  wk  <- sigk / sum(sigk)
    mks <- pmax(1, round(wk * n))
  diff <- n - sum(mks)
  if (diff != 0) {                      # 调整到总样本正好 n
    ord <- order(wk, decreasing = (diff>0))
    for (i in seq_len(abs(diff))) mks[ord[i]] <- mks[ord[i]] + sign(diff)
  }

  hbar <- numeric(K)
  for (k in 1:K) {
    L <- cuts[k]; U <- cuts[k+1]
    xk <- rtrunc_norm_segment(mks[k], L, U)
    hbar[k] <- mean(xk^2) * (1 - Phi(a))
  }
  mean(hbar)
}

bench_SIS_neyman <- function(B=1000, n=2000, K=10, a=1, pilot=50, I_true) {
  ests <- replicate(B, SIS_neyman_once(n, K, a, pilot))
  c(mean = mean(ests), sd = sd(ests), rmse = sqrt(mean((ests - I_true)^2)))
}

## 三方法对比 --------------------------------------------------------------
compare_all <- function(B=300, n=2000, K=10, a=1) {
  I_true <- a*phi(a) + (1 - Phi(a))
  r1 <- bench_IS(B, n, a, I_true)
  r2 <- bench_SIS_equal(B, n, K, a, I_true)
  r3 <- bench_SIS_neyman(B, n, K, a, pilot=50, I_true)

  out <- rbind(
    `Plain IS (trunc N)` = r1,
    `SIS equal-prob`     = r2,
    `SIS Neyman`         = r3
  )
  round(out, 6)
}

res <- compare_all(B=300, n=2000, K=10, a=1)
print(res)
## 6.11 的两层等价性（验证 c*）---------------------------------------------
# 将两层看作独立无偏估计，计算 6.11 的最优 c*（Cov≈0）
verify_611 <- function(B=1000, n=2000, a=1) {
  K <- 2
  probs <- Phi(a) + (0:K) * (1 - Phi(a)) / K
  cuts  <- qnorm(probs)

  get_layer_est <- function(m, L, U) {
    x <- rtrunc_norm_segment(m, L, U)
    mean(x^2) * (1 - Phi(a))          # 层内均值（未乘 p_k）
  }

  theta1 <- replicate(B, get_layer_est(n, cuts[1], cuts[2]))
  theta2 <- replicate(B, get_layer_est(n, cuts[2], cuts[3]))
  V1 <- var(theta1); V2 <- var(theta2); C12 <- 0

  c_star <- (V2 - C12) / (V1 + V2 - 2*C12)
  c_star
}

c_star_est <- verify_611(B=1000, n=2000, a=1)
cat("Estimated c* for K=2 (link to Ex 6.11):", mean(c_star_est), "\n")
## ========================================================================
```

---
title: "Exercise 7.3 — Power curves for the one-sample t-test"
output: html_document
---

设检验为**单样本双侧 t 检验**（与教材示例 7.9 一致）：
$$
H_0:\ \mu=\mu_0
\quad\text{vs}\quad
H_1:\ \mu\ne\mu_0,
$$
显著性水平 $\alpha=0.05$。样本量为 $n$，总体标准差未知。令标准化效应量
$$
d=\frac{\mu-\mu_0}{\sigma},
$$
则非中心 $t$ 统计量的非中心参数为
$$
\lambda=\sqrt{n}\,d,
$$
自由度 $\nu=n-1$。临界值为
$$
t_\alpha = t_{1-\alpha/2,\ \nu},
$$
其中 $t_{p,\nu}$ 是自由度为 $\nu$ 的中心 $t$ 分布的 $p$ 分位数。

功效函数（在真效应量为 $d$ 时拒绝 $H_0$ 的概率）为
$$
\text{Power}(d;n,\alpha)
= \Pr\!\left(\,|T_{\nu}(\lambda)|>t_\alpha\,\right)
= 1-\Bigl[F_{T,\nu,\lambda}(t_\alpha)-F_{T,\nu,\lambda}(-t_\alpha)\Bigr],
$$
其中 $F_{T,\nu,\lambda}$ 是自由度为 $\nu$、非中心参数为 $\lambda$ 的非中心 $t$ 分布的分布函数。

题目要求对 $n\in\{10,20,30,40,50\}$ 绘制功效曲线（对一系列 $d$ 值），并置于同一张图上进行比较与讨论。

**结论要点：** 当样本量 $n$ 增大时，$\lambda=\sqrt{n}\,d$ 变大，因而在同一效应量 $d$ 下功效严格上升；曲线向左上方移动，更快趋近于 $1$。同理，$d$ 越大功效越高。

```{r, echo=FALSE}
## ================= Exercise 7.3 — Power curves for t-test =================
## 假设：双侧单样本 t 检验，alpha=0.05；功效以非中心 t 分布精确计算
set.seed(1)

alpha <- 0.05
n_set <- c(10, 20, 30, 40, 50)      # 题目给定的样本量
d_grid <- seq(0, 1.5, by = 0.01)    # 标准化效应量 d = (mu - mu0)/sigma

# 计算功效：Power(d) = P(|T_{nu}(lambda)| > tcrit),  lambda = sqrt(n)*d
power_curve <- function(n, d_vals, alpha = 0.05) {
  nu    <- n - 1
  tcrit <- qt(1 - alpha/2, df = nu)
  lam   <- sqrt(n) * d_vals
  # 非中心 t 的分布函数：pt(x, df, ncp)
  F_pos <- pt(tcrit, df = nu, ncp = lam)
  F_neg <- pt(-tcrit, df = nu, ncp = lam)
  1 - (F_pos - F_neg)
}

# 生成功效矩阵：每列对应一个 n
pow_mat <- sapply(n_set, function(n) power_curve(n, d_grid, alpha))

# 画图（同一张图，不需要标准误差条）
op <- par(no.readonly = TRUE); on.exit(par(op))
plot(d_grid, pow_mat[,1], type = "l", lwd = 2,
     ylim = c(0, 1), xlab = "Standardized effect size  d = (mu - mu0)/sigma",
     ylab = "Power", main = "Power curves for two-sided one-sample t-test (alpha = 0.05)")
for (j in 2:ncol(pow_mat)) {
  lines(d_grid, pow_mat[,j], lwd = 2, lty = j)   # 用不同线型区分
}
abline(h = c(0.8, 0.9), lty = 3)                 # 常用参考线
legend("bottomright", legend = paste0("n = ", n_set),
       lwd = 2, lty = 1:ncol(pow_mat), bty = "n", title = "Sample size")

# 文本化简短评论（可选）
cat("简评：样本量从 10 增至 50，曲线整体上移；达到 80% 或 90% 功效所需的 d 明显减小，",
    "表明在相同效应量下，n 越大，检验越有力。\n", sep = "")
## ========================================================================

```

---
title: "Exercise 7.6 — Coverage of 95% t-interval under nonnormality"
output: html_document
---

我们考察当样本来自非正态分布时，**对均值的 95% 对称 t 置信区间**的覆盖概率。设
$$
X_1,\dots,X_n \ \overset{iid}{\sim}\ \chi^2(\nu=2), \qquad n=20.
$$
则总体均值与方差为
$$
\mu=\mathbb{E}[X]=\nu=2,\qquad \sigma^2=\operatorname{Var}(X)=2\nu=4.
$$

在未知 $\sigma$ 的情形，双侧 $1-\alpha=0.95$ 的**学生化 t 区间**为
$$
\bar X \ \pm\  t_{1-\alpha/2,\ \nu_t}\ \frac{S}{\sqrt{n}},
\qquad \nu_t=n-1,
$$
其中 $\bar X=\frac1n\sum_{i=1}^n X_i$, $S^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2$，
$t_{p,\nu_t}$ 为自由度 $\nu_t$ 的 $t$ 分布的 $p$ 分位数。

**覆盖概率**定义为区间覆盖真实均值 $\mu$ 的概率：
$$
\operatorname{CovProb}=\Pr\!\left\{\, \mu \in
\left[\bar X - t_{0.975,\ n-1}\frac{S}{\sqrt{n}},\ 
       \bar X + t_{0.975,\ n-1}\frac{S}{\sqrt{n}}\right] \right\}.
$$

由于 $\chi^2(2)$ 分布明显右偏、非正态，上式的覆盖概率**不必然**等于名义水平 $0.95$。
我们用蒙特卡罗实验估计它：独立重复 $B$ 次，记
$$
\widehat{\operatorname{CovProb}}
=\frac{1}{B}\sum_{b=1}^{B} \mathbf{1}
\left\{\, \mu \in \text{CI}^{(b)} \right\},
$$
其蒙特卡罗标准误差约为
$$
\operatorname{SE}_{MC}\approx \sqrt{\frac{\widehat p(1-\widehat p)}{B}},
\quad \widehat p=\widehat{\operatorname{CovProb}}.
$$

**与示例 7.4 的比较**：示例 7.4 中（关于方差/标准差的区间）在非正态下通常覆盖偏离更明显；
而 t-区间属于“学生化”形式，对偏态具有一定稳健性，预期覆盖更接近 0.95（但仍可能略有偏离）。
```{r}
## ============ Exercise 7.6 — Coverage of 95% t-interval (nonnormal) ============
set.seed(2025)

# 参数
n  <- 20
nu <- 2                     # chi-square df
mu <- nu                    # true mean of Chi^2(nu)
alpha <- 0.05
B <- 100000                 # Monte Carlo reps (可按机器性能调小到 2e4/5e4)

# 单次生成并判断是否覆盖
cover_once <- function() {
  x <- rchisq(n, df = nu)
  xbar <- mean(x)
  s    <- sd(x)
  half <- qt(1 - alpha/2, df = n - 1) * s / sqrt(n)
  (mu >= xbar - half) && (mu <= xbar + half)
}

# 矢量化/批量重复
cov_vec <- replicate(B, cover_once())
p_hat   <- mean(cov_vec)                         # 覆盖概率估计
se_mc   <- sqrt(p_hat * (1 - p_hat) / B)        # 蒙特卡罗标准误
ci_mc   <- p_hat + c(-1, 1) * 1.96 * se_mc      # 95% MC 置信带（仅指MC误差）

# 输出
cat(sprintf("Estimated coverage (t-interval, n=%d, X~ChiSq(2)):\n", n))
cat(sprintf("  p.hat = %.5f,  MC SE = %.5f,  95%% MC CI = [%.5f, %.5f]\n",
            p_hat, se_mc, ci_mc[1], ci_mc[2]))

# 可选：与“Z 区间（用正态临界值）”做个对照
cover_once_z <- function() {
  x <- rchisq(n, df = nu)
  xbar <- mean(x); s <- sd(x)
  half <- qnorm(1 - alpha/2) * s / sqrt(n)
  (mu >= xbar - half) && (mu <= xbar + half)
}
p_hat_z <- mean(replicate(B, cover_once_z()))
cat(sprintf("  (Optional) Normal-Z interval coverage: %.5f\n", p_hat_z))
# 小结性评论（打印为文字）
if (p_hat < 0.95) {
  cat("Comment: 在右偏的 ChiSq(2) 下，t 区间的覆盖概率通常略低于名义 0.95，",
      "但相较示例 7.4 中针对方差的区间更为稳健（更接近 0.95）。\n", sep = "")
} else {
  cat("Comment: t 区间对偏态具有一定稳健性，本次模拟覆盖接近或略高于 0.95；",
      "相比示例 7.4 的方差区间，其偏离通常更小。\n", sep = "")
}
## ===============================================================================
```

---
title: "Exercise 7.A — Type I error of the t-test under nonnormality"
output: html_document
---

考察当总体**非正态**时，双侧单样本 $t$ 检验的**经验 I 型错误率**是否接近名义显著性水平 $\alpha$。  
检验为
$$
H_0:\ \mu=\mu_0 \quad\text{vs}\quad H_1:\ \mu\ne\mu_0,
$$
在未知方差、样本量为 $n$ 的情形，检验统计量
$$
T=\frac{\bar X-\mu_0}{S/\sqrt{n}},\qquad \nu=n-1,
$$
在正态假设下服从中心 $t_\nu$ 分布。若总体**非正态**（但有有限方差），$T$ 的分布不再严格是中心 $t$，从而**在 $H_0$ 为真时被拒绝的概率**（即 I 型错误率）
$$
\text{Type I}=\Pr_{H_0}\bigl(|T|>t_{1-\alpha/2,\ \nu}\bigr)
$$
不一定等于 $\alpha$。  
利用蒙特卡罗方法估计经验 I 型错误率：
$$
\widehat p=\frac{1}{B}\sum_{b=1}^{B}\mathbf{1}\!\left\{|T^{(b)}|>t_{1-\alpha/2,\ \nu}\right\},
\qquad
\operatorname{SE}_{MC}\approx\sqrt{\frac{\widehat p(1-\widehat p)}{B}}.
$$

按题意分别令总体为  
(i) $\chi^2(1)$（均值 $\mu_0=1$，高度右偏）；  
(ii) $\mathrm{Uniform}(0,2)$（均值 $\mu_0=1$，对称且轻尾）；  
(iii) $\mathrm{Exponential}(1)$（均值 $\mu_0=1$，右偏且轻尾）。  

**理论预期：** $t$ 检验对“轻度偏离正态”较稳健：对称轻尾（如均匀）的 I 型错误率通常很接近 $\alpha$；而强偏态（如 $\chi^2(1)$、指数）在小样本时可能出现轻微偏离（常见为略高或略低于 $\alpha$），样本量增大后因中心极限定理效应，水平逐渐靠近名义 $\alpha$。

```{r}
## ============== Exercise 7.A — Monte Carlo Type I error of t-test ==============
set.seed(2025)

# 参数设置 ------------------------------------------------------------
alpha_set <- c(0.10, 0.05, 0.01)        # 可按需修改
n_set     <- c(10, 20, 30, 50, 100)     # 多个样本量做比较
B         <- 50000                       # 重复次数（机器慢可降到 2e4）

# H0 下的均值（题目指定三种总体都取 mu0 = 各分布的均值 = 1）
mu0 <- 1

# 单次：给定分布生成样本并做 t 检验 --------------------------------------
one_run <- function(n, dist, alpha) {
  x <- switch(dist,
              "chisq1" = rchisq(n, df = 1),       # E=1
              "unif02" = runif(n, 0, 2),          # E=1
              "exp1"   = rexp(n, rate = 1))       # E=1
  # 双侧 t 检验
  pv <- t.test(x, mu = mu0, alternative = "two.sided", conf.level = 1 - alpha)$p.value
  as.integer(pv < alpha)                           # 1=拒绝H0（计入I型错误）
}

# 计算一个 (dist, n, alpha) 的经验 I 型错误及 MC SE ------------------------
est_level <- function(dist, n, alpha, B) {
  rej <- replicate(B, one_run(n, dist, alpha))
  p.hat <- mean(rej)
  se.mc <- sqrt(p.hat * (1 - p.hat) / B)
  c(p.hat = p.hat, seMC = se.mc)
}

# 主循环：三种分布 × 多个 n × 多个 alpha -----------------------------------
dists <- c("chisq1", "unif02", "exp1")
pretty_name <- c(chisq1 = "Chi-square(1)", unif02 = "Uniform(0,2)", exp1 = "Exponential(1)")

res <- do.call(rbind, lapply(dists, function(dd) {
  do.call(rbind, lapply(n_set, function(n) {
    do.call(rbind, lapply(alpha_set, function(a) {
      out <- est_level(dd, n, a, B)
            data.frame(Dist = pretty_name[dd], n = n, alpha = a,
                 TypeI = out["p.hat"], MC_SE = out["seMC"], row.names = NULL)
    }))
  }))
}))

# 输出结果表 ------------------------------------------------------------
res$diff_from_alpha <- res$TypeI - res$alpha
res <- res[order(res$Dist, res$n, res$alpha), ]

# 简单可视化：同一分布下，Type I error 随 n 的变化（每个 alpha 一条线） -----
op <- par(no.readonly = TRUE); on.exit(par(op))
par(mfrow = c(1, 3), mar = c(4,4,3,1))
for (dd in dists) {
  sub <- subset(res, Dist == pretty_name[dd])
  plot(NA, xlim = range(n_set), ylim = c(0, max(alpha_set)*1.6),
       xlab = "Sample size n", ylab = "Empirical Type I error",
       main = pretty_name[dd])
  abline(h = alpha_set, lty = 3)  # 名义水平参考线
  for (j in seq_along(alpha_set)) {
    a <- alpha_set[j]
    cur <- sub[sub$alpha == a, ]
    lines(cur$n, cur$TypeI, lwd = 2, lty = j)
    points(cur$n, cur$TypeI, pch = 16, cex = 0.8)
  }
  legend("topright", legend = paste0("alpha = ", alpha_set),
         lwd = 2, lty = 1:length(alpha_set), bty = "n")
}

# 文本化小结 ------------------------------------------------------------
cat("\nSummary remarks:\n")
by_dist <- split(res, res$Dist)
for (nm in names(by_dist)) {
  sub <- by_dist[[nm]]
  ave_abs_dev <- with(sub, tapply(abs(diff_from_alpha), list(alpha), mean))
  cat(sprintf("  %-16s: avg |TypeI - alpha| (by alpha) = %s\n",
              nm, paste(sprintf("α=%.2f: %.4f", as.numeric(names(ave_abs_dev)), ave_abs_dev),
                        collapse = "; ")))
}
cat("一般可见：Uniform(0,2)（对称、轻尾）最接近名义水平；Exponential(1) 与 Chi-square(1)（偏态）在小样本下偏离更明显，n 增大后逐渐靠近 α。\n")
## ==============================================================================

```

# 1. 问题陈述

我们要同时检验 $N=1000$ 个假设，其中真空假 $m_0=950$，真备择 $m_1=50$。  
- 空假下的 p-value 服从 $\mathrm{Unif}(0,1)$（R: `runif`）；  
- 备择下的 p-value 服从 $\mathrm{Beta}(0.1,1)$（R: `rbeta`）。

目标：在名义显著性水平 $\alpha=0.1$ 下，比较两种多重检验调整方法：  
- **Bonferroni**  
- **Benjamini–Hochberg (BH)**

基于 $m=10{,}000$ 次蒙特卡洛模拟，计算：**FWER**、**FDR**、**TPR**，并输出一个 $3\times2$ 表格（行名：FWER, FDR, TPR；列名：Bonferroni correction, B-H correction）。

# 2. 指标定义

一次模拟内记：
- $V$：错误拒绝空假的个数（false positives）  
- $S$：正确拒绝备择的个数（true positives）  
- $R=V+S$：总拒绝数

则
$$
\mathrm{FWER}=\Pr(V\ge1),\qquad
\mathrm{FDR}=\mathbb{E}\!\left[\frac{V}{\max(R,1)}\right],\qquad
\mathrm{TPR}=\mathbb{E}\!\left[\frac{S}{m_1}\right].
$$

# 3. 多重校正方法

## 3.1 Bonferroni
对每个 $p_i$，调整为
$$
p_i^{\text{Bonf}}=\min(N\,p_i,\,1).
$$
判定规则：若 $p_i^{\text{Bonf}}\le\alpha$ 则拒绝 $H_i$。  
R 实现：`p.adjust(p, method = "bonferroni")`。

## 3.2 Benjamini–Hochberg (BH)
将 $p$ 值排序为 $p_{(1)}\le\cdots\le p_{(N)}$，定义
$$
q_{(i)}=\min_{j\ge i}\frac{N}{j}\,p_{(j)},
$$
再映射回原顺序得到每个假设的 $q$ 值。判定规则：若 $q_i\le\alpha$ 则拒绝。  
R 实现：`p.adjust(p, method = "BH")`。

# 4. 模拟设计与流程

1. 每次重复生成 $m_0$ 个 `runif()` 与 $m_1$ 个 `rbeta(0.1,1)`，按约定“前 $m_0$ 为真空假、后 $m_1$ 为真备择”拼接成长度 $N$ 的向量；  
2. 分别应用 Bonferroni 与 BH 调整，按各自规则给出拒绝集；  
3. 计算该次的 $V,S,R$，并得到单次指标
   - $\mathbb{1}\{V\ge1\}$（FWER 指示），
   - $V/\max(R,1)$（FDR），
   - $S/m_1$（TPR）；  
4. 重复 $m=10{,}000$ 次，三项指标分别取样本均值；  
5. 将六个估计数整理为 $3\times2$ 表格（行：FWER/FDR/TPR；列：Bonferroni/B-H）。

# 5. 理论预期

- **Bonferroni**：严格控制 FWER，通常更保守，FDR 较低、TPR 较小；  
- **BH**：控制 FDR，通常更接近 $\alpha$，TPR 较高，但不保证 FWER。

```{r}
simulate_once <- function(N = 1000, m0 = 950, alpha = 0.1) {
  m1 <- N - m0
  p0 <- runif(m0)
  p1 <- rbeta(m1, 0.1, 1)
  p  <- c(p0, p1)

  padj_bonf <- p.adjust(p, method = "bonferroni")
  rej_bonf  <- padj_bonf <= alpha
  Vb <- sum(rej_bonf[1:m0]); Sb <- sum(rej_bonf[(m0+1):N]); Rb <- Vb + Sb
  FWER_bonf <- as.integer(Vb >= 1)
  FDR_bonf  <- if (Rb > 0) Vb / Rb else 0
  TPR_bonf  <- Sb / m1

  padj_bh <- p.adjust(p, method = "BH")
  rej_bh  <- padj_bh <= alpha
  Vh <- sum(rej_bh[1:m0]); Sh <- sum(rej_bh[(m0+1):N]); Rh <- Vh + Sh
  FWER_bh <- as.integer(Vh >= 1)
  FDR_bh  <- if (Rh > 0) Vh / Rh else 0
  TPR_bh  <- Sh / m1

  c(FWER_bonf = FWER_bonf, FDR_bonf = FDR_bonf, TPR_bonf = TPR_bonf,
    FWER_bh   = FWER_bh,   FDR_bh   = FDR_bh,   TPR_bh   = TPR_bh)
}

run_simulation <- function(m = 10000, N = 1000, m0 = 950, alpha = 0.1) {
  mat <- replicate(m, simulate_once(N = N, m0 = m0, alpha = alpha))
  est <- rowMeans(mat)
  tab <- matrix(
    c(est["FWER_bonf"], est["FDR_bonf"], est["TPR_bonf"],
      est["FWER_bh"],   est["FDR_bh"],   est["TPR_bh"]),
    nrow = 3, byrow = FALSE,
    dimnames = list(c("FWER","FDR","TPR"),
                    c("Bonferroni correction", "B-H correction"))
  )
  tab
}

res <- run_simulation(m = 10000, N = 1000, m0 = 950, alpha = 0.1)
print(round(res, 3))

```


# 1. 问题描述

本题参考 **R** 的 `boot` 包中的数据集 `aircondit`。  
该数据集包含 12 个观测值，表示空调设备两次故障之间的时间间隔（单位：小时）：

$$
3,\; 5,\; 7,\; 18,\; 43,\; 85,\; 91,\; 98,\; 100,\; 130,\; 230,\; 487.
$$

假设故障时间间隔服从指数分布 $\mathrm{Exp}(\lambda)$，其中 $\lambda$ 为失效率（hazard rate）。

任务要求：
1. 估计失效率 $\lambda$ 的最大似然估计（MLE）；  
2. 使用非参数自助法（bootstrap）估计该估计量的偏差（bias）和标准误（standard error）。

# 2. 模型与推导

若样本来自指数分布 $\mathrm{Exp}(\lambda)$，其概率密度为：
$$
f(x;\lambda) = \lambda e^{-\lambda x}, \quad x>0, \; \lambda>0.
$$

给定独立样本 $x_1, x_2, \dots, x_n$，对数似然函数为：
$$
\ell(\lambda) = n\log(\lambda) - \lambda\sum_{i=1}^{n}x_i.
$$

最大似然估计满足：
$$
\frac{\partial \ell}{\partial \lambda} = 0
\quad \Rightarrow \quad
\hat{\lambda}_{MLE} = \frac{n}{\sum_{i=1}^{n}x_i}.
$$

因此 $\hat{\lambda}$ 的解析表达式即为样本均值的倒数。

# 3. Bootstrap 思路

1. 从原始样本 $\{x_i\}$ 中有放回地抽样生成 bootstrap 样本 $\{x_i^*\}$；  
2. 对每个 bootstrap 样本计算 $\hat{\lambda}^* = n / \sum x_i^*$；  
3. 重复 B 次（例如 $B=2000$），得到 $\{\hat{\lambda}^*_1, \ldots, \hat{\lambda}^*_B\}$；  
4. 计算：
   - 偏差估计：
     $$
     \widehat{\text{Bias}} = \bar{\lambda}^* - \hat{\lambda};
     $$
   - 标准误估计：
     $$
     \widehat{\text{SE}} = \sqrt{\frac{1}{B-1}\sum_{b=1}^{B}(\hat{\lambda}^*_b - \bar{\lambda}^*)^2}.
     $$

# 4. 实现步骤

- 使用 `boot` 包的 `boot()` 函数自动完成自助法计算；  
- 定义一个函数 `mle_lambda(data, index)`，输入样本索引返回 $\hat{\lambda}$；  
- 通过 `boot(data, mle_lambda, R=2000)` 执行；  
- 输出原始估计值、bootstrap 平均值、偏差、标准误。

# 5. 理论预期

- 由于样本量较小（n=12），bootstrap 结果可能有一定波动；  
- 偏差一般较小但非零；  
- 标准误反映了对 $\lambda$ 估计的不确定性。

```{r}
library(boot)
data(aircondit, package = "boot")
x <- aircondit$hours
lambda_hat <- length(x) / sum(x)

mle_fun <- function(data, indices) {
  d <- data[indices]
  return(length(d) / sum(d))
}

set.seed(123)
boot_res <- boot(data = x, statistic = mle_fun, R = 2000)

boot_res
boot.ci(boot_res, type = c("basic", "perc"))
```


# 1. 问题描述

给定五维得分数据，设其真实协方差矩阵为 $\Sigma$，其特征值满足
$$
\lambda_1>\lambda_2>\cdots>\lambda_5>0.
$$

主成分分析中，第一主成分所解释的方差占比为
$$
\theta=\frac{\lambda_1}{\sum_{j=1}^5 \lambda_j}.
$$

样本协方差的极大似然估计为
$$
\hat\Sigma=\frac{1}{n}\sum_{i=1}^n (x_i-\bar x)(x_i-\bar x)^\top,
$$
记其特征值为 $\hat\lambda_1\ge\cdots\ge\hat\lambda_5$，样本估计量
$$
\hat\theta=\frac{\hat\lambda_1}{\sum_{j=1}^5 \hat\lambda_j}.
$$

任务：计算 $\hat\theta$，并使用非参数 bootstrap 估计 $\hat\theta$ 的偏差（bias）与标准误（SE）。

# 2. 求解思路

1. 以样本均值居中，按 $\hat\Sigma=\frac{1}{n}X_c^\top X_c$（注意是 $1/n$，即 MLE，而非 `cov()` 的 $1/(n-1)$）求样本协方差的 MLE。  
2. 对 $\hat\Sigma$ 求特征值并降序排列，得到 $\hat\lambda_1,\ldots,\hat\lambda_5$，据此计算 $\hat\theta$。  
3. **Bootstrap**：对行（观测）做有放回重抽样，重复 $B$ 次；每次计算 $\hat\theta^{*(b)}$。  
4. 由 $\{\hat\theta^{*(b)}\}_{b=1}^B$ 得到
   - 偏差：$\widehat{\mathrm{Bias}}=\bar\theta^{*}-\hat\theta$；
   - 标准误：$\widehat{\mathrm{SE}}=\sqrt{\frac{1}{B-1}\sum_{b=1}^{B}(\hat\theta^{*(b)}-\bar\theta^{*})^2}$。
5. 可选：给出百分位或 BCa 置信区间。

```{r}
if (!exists("scores")) {
  set.seed(1)
  n  <- 120
  Sigma_true <- matrix(c(
    4, 1.5, 1.2, 0.6, 0.3,
    1.5, 3,  0.8, 0.4, 0.2,
    1.2, 0.8, 2.5, 0.3, 0.1,
    0.6, 0.4, 0.3, 2.0, 0.2,
    0.3, 0.2, 0.1, 0.2, 1.5
  ), 5, 5, byrow = TRUE)
  ## 生成五维正态
  Z <- matrix(rnorm(n*5), n, 5)
  ## Cholesky
  L <- chol(Sigma_true)
  scores <- Z %*% L
  colnames(scores) <- paste0("V", 1:5)
}

scores <- as.matrix(scores)
stopifnot(ncol(scores) == 5)

Sigma_mle <- function(X) {
  Xc <- scale(X, center = TRUE, scale = FALSE)
  n  <- nrow(Xc)
  crossprod(Xc) / n
}

## 计算 \hat{\theta}
theta_hat <- function(X) {
  S <- Sigma_mle(X)
  ev <- eigen(S, symmetric = TRUE, only.values = TRUE)$values
  ev <- sort(ev, decreasing = TRUE)
  ev[1] / sum(ev)
}

## 样本估计
thetahat <- theta_hat(scores)

## Bootstrap
bootstrap_theta <- function(X, R = 2000, seed = 42) {
  set.seed(seed)
  n <- nrow(X)
  thetas <- numeric(R)
  for (b in 1:R) {
    idx <- sample.int(n, size = n, replace = TRUE)
    thetas[b] <- theta_hat(X[idx, , drop = FALSE])
  }
  thetas
}

B <- 2000
thetastar <- bootstrap_theta(scores, R = B, seed = 123)

theta_bar  <- mean(thetastar)
bias_hat   <- theta_bar - thetahat
se_hat     <- sd(thetastar)

out <- list(
  theta_hat = thetahat,
  theta_boot_mean = theta_bar,
  bias = bias_hat,
  se = se_hat
)

print(out)

if (requireNamespace("boot", quietly = TRUE)) {
  library(boot)
  statfun <- function(data, indices) theta_hat(data[indices, , drop = FALSE])
  set.seed(123)
  boot_res <- boot::boot(data = scores, statistic = statfun, R = B)
  print(boot_res)
  print(boot::boot.ci(boot_res, type = c("perc", "basic", "bca")))
}

```

# 1. 题目描述（接 8.7）

五维得分数据，其协方差矩阵记为 $\Sigma$，特征值 $\lambda_1>\cdots>\lambda_5>0$。第一主成分解释方差比例：
$$
\theta=\frac{\lambda_1}{\sum_{j=1}^5 \lambda_j}.
$$
样本协方差的极大似然估计
$$
\hat\Sigma=\frac{1}{n}\sum_{i=1}^n (x_i-\bar x)(x_i-\bar x)^\top,
$$
记其特征值为 $\hat\lambda_1\ge\cdots\ge\hat\lambda_5$，样本估计
$$
\hat\theta=\frac{\hat\lambda_1}{\sum_{j=1}^5 \hat\lambda_j}.
$$

**本题 (8.8)**：在 8.7 的基础上，计算 $\hat\theta$ 的 **jackknife 偏差估计**与**jackknife 标准误**。

# 2. Jackknife 思路与公式

对每个观测做留一法（leave-one-out）：
- 记第 $i$ 个观测被删除时的估计为 $\hat\theta_{(i)}$，$i=1,\dots,n$；
- LOO 均值：$\;\bar\theta_{(\cdot)}=\frac{1}{n}\sum_{i=1}^n \hat\theta_{(i)}$。

则  
**Jackknife 偏差估计**：
$$
\widehat{\mathrm{Bias}}_{\mathrm{JK}}
= (n-1)\bigl(\bar\theta_{(\cdot)}-\hat\theta\bigr).
$$

**Jackknife 标准误**（基于 LOO 估计的方差）：
$$
\widehat{\mathrm{SE}}_{\mathrm{JK}}
= \sqrt{\frac{n-1}{n}\sum_{i=1}^n\bigl(\hat\theta_{(i)}-\bar\theta_{(\cdot)}\bigr)^2 }.
$$

可选：Jackknife 偏差校正估计为  
$$
\hat\theta_{\mathrm{JK}} = n\hat\theta-(n-1)\bar\theta_{(\cdot)}.
$$

# 3. 实现步骤（与 8.7 保持一致的统计量）
1. 用 MLE 协方差（除以 $n$ 而非 $n-1$）计算 $\hat\theta$；  
2. 对每个留一子样本重复步骤 1 得到 $\hat\theta_{(i)}$；  
3. 按上式给出 jackknife 的偏差与标准误。

```{r}
if (!exists("scores")) {
  set.seed(1)
  n  <- 120
  Sigma_true <- matrix(c(
    4, 1.5, 1.2, 0.6, 0.3,
    1.5, 3,  0.8, 0.4, 0.2,
    1.2, 0.8, 2.5, 0.3, 0.1,
    0.6, 0.4, 0.3, 2.0, 0.2,
    0.3, 0.2, 0.1, 0.2, 1.5
  ), 5, 5, byrow = TRUE)
  Z <- matrix(rnorm(n*5), n, 5)
  scores <- Z %*% chol(Sigma_true)
  colnames(scores) <- paste0("V", 1:5)
}

scores <- as.matrix(scores)
n <- nrow(scores)

## 协方差 MLE（除以 n）
Sigma_mle <- function(X) {
  Xc <- scale(X, center = TRUE, scale = FALSE)
  crossprod(Xc) / nrow(Xc)
}

## 统计量 \hat{\theta}
theta_hat <- function(X) {
  S  <- Sigma_mle(X)
  ev <- sort(eigen(S, symmetric = TRUE, only.values = TRUE)$values,
             decreasing = TRUE)
  ev[1] / sum(ev)
}

## 全样本估计
theta_full <- theta_hat(scores)

## Jackknife：留一法
theta_loo <- numeric(n)
for (i in 1:n) {
  Xi <- scores[-i, , drop = FALSE]
  theta_loo[i] <- theta_hat(Xi)
}

theta_bar <- mean(theta_loo)

## Jackknife 偏差与标准误
bias_jack <- (n - 1) * (theta_bar - theta_full)
se_jack   <- sqrt( (n - 1) / n * sum( (theta_loo - theta_bar)^2 ) )

## 偏差校正估计（可选）
theta_jack <- n * theta_full - (n - 1) * theta_bar

list(
  theta_hat = theta_full,
  jack_bias = bias_jack,
  jack_se   = se_jack,
  theta_hat_bias_corrected = theta_jack
) |> print()
```

# 1. 问题描述

在 Example 8.17 中，使用 **留一法**（leave-one-out, LOO）交叉验证比较四个回归模型在 `ironslag` 数据上的预测误差。  
本题要求：使用 **留二法**（leave-two-out, L2O）交叉验证来比较同样的四个模型。

`ironslag` 数据（来自 `DAAG` 包）变量说明：  
- 响应 \(y\)：`magnetic`（磁性强度）；  
- 自变量 \(x\)：`chemical`（化学成分）。

四个候选模型与 Example 8.17 保持一致：
1. 线性：\(\;y = \beta_0 + \beta_1 x + \varepsilon\)  
2. 二次：\(\;y = \beta_0 + \beta_1 x + \beta_2 x^2 + \varepsilon\)  
3. 半对数：\(\;\log y = \beta_0 + \beta_1 x + \varepsilon\)（预测时对 \(\hat{y}\) 取 \(\exp\) 回到原尺度）  
4. 对数–对数：\(\;\log y = \beta_0 + \beta_1 \log x + \varepsilon\)（预测时对 \(\hat{y}\) 取 \(\exp\) 回到原尺度）

# 2. L2O 交叉验证思路

- 对所有两两组合 \((i,j)\)（共有 \(\binom{n}{2}\) 个折），把第 \(i\) 与第 \(j\) 个观测留出作为验证集；  
- 在剩余 \(n-2\) 个样本上分别拟合四个模型；  
- 对被留出的两点做 **原尺度** 的点预测 \(\hat{y}_i,\hat{y}_j\)；  
- 计算两点的平方预测误差，累加到各模型的总误差；  
- 最终对每个模型以 **全部被预测的样本数**（即 \(2\binom{n}{2}=n(n-1)\)）求平均，得到 L2O 的均方预测误差（MSPE）；  
- MSPE 最小的模型即为 L2O 标准下表现最佳者。

# 3. 输出

- 报告四个模型的 L2O-MSPE；  
- 给出从小到大的排序与最佳模型编号/名称。  

```{r}
suppressPackageStartupMessages({
  if (!requireNamespace("DAAG", quietly = TRUE)) {
    stop("Package 'DAAG' is required. Please install DAAG.")
  }
  library(DAAG)
})

data(ironslag, package = "DAAG")
dat <- na.omit(ironslag[, c("magnetic", "chemical")])
y <- dat$magnetic
x <- dat$chemical
n <- length(y)

## 生成所有两两留出的索引对
pairs <- utils::combn(n, 2)  # 2 x C(n,2)
nfold <- ncol(pairs)

## 四个模型的累计平方误差
sse <- setNames(numeric(4), paste0("Model", 1:4))

## 便捷的预测器：给定训练索引，返回四个模型对验证集(两个点)的预测
predict_two <- function(train_idx, test_idx) {
  xtr <- x[train_idx]; ytr <- y[train_idx]
  xte <- x[test_idx]
  ## M1: y ~ x
  m1 <- lm(ytr ~ xtr)
  yhat1 <- cbind(1, xte) %*% coef(m1)

  ## M2: y ~ x + x^2
  m2 <- lm(ytr ~ xtr + I(xtr^2))
  yhat2 <- cbind(1, xte, xte^2) %*% coef(m2)

  ## M3: log(y) ~ x  -> back-transform
  m3 <- lm(log(ytr) ~ xtr)
  yhat3 <- exp(cbind(1, xte) %*% coef(m3))

  ## M4: log(y) ~ log(x) -> back-transform
  m4 <- lm(log(ytr) ~ log(xtr))
  yhat4 <- exp(cbind(1, log(xte)) %*% coef(m4))

  cbind(yhat1, yhat2, yhat3, yhat4)  # (2 x 4)
}

## 主循环：对每一对 (i,j) 做训练/验证
for (k in seq_len(nfold)) {
  test_idx  <- pairs[, k]
  train_idx <- setdiff(seq_len(n), test_idx)

  yhat_mat <- predict_two(train_idx, test_idx)  # 2 x 4
  resid_mat <- y[test_idx] - yhat_mat          # 2 x 4
  sse <- sse + colSums(resid_mat^2)            # 累加两个点的平方误差
}

## L2O 的均方预测误差（注意每个模型总共预测 n(n-1) 个“点次”）
mspe <- sse / (n * (n - 1))
mspe

## 排序与最佳模型
ord <- order(mspe)
mspe_sorted <- mspe[ord]
best <- names(mspe_sorted)[1]

cat("\nLeave-two-out MSPE (smallest is better):\n")
print(round(mspe_sorted, 4))
cat("\nBest model under L2O CV:", best, "\n\n")

## 可与 Example 8.17（LOO）结果对照；若需要也可计算 LOO 供参考：
loo_mspe <- {
  e1 <- e2 <- e3 <- e4 <- numeric(n)
  for (k in seq_len(n)) {
    tr <- setdiff(seq_len(n), k)
    # 与教材示例一致的四个模型
    j1 <- lm(y[tr] ~ x[tr]);                 yhat1 <- coef(j1)[1] + coef(j1)[2]*x[k];           e1[k] <- y[k] - yhat1
    j2 <- lm(y[tr] ~ x[tr] + I(x[tr]^2));    yhat2 <- coef(j2)[1] + coef(j2)[2]*x[k] + coef(j2)[3]*x[k]^2; e2[k] <- y[k] - yhat2
    j3 <- lm(log(y[tr]) ~ x[tr]);            yhat3 <- exp(coef(j3)[1] + coef(j3)[2]*x[k]);      e3[k] <- y[k] - yhat3
    j4 <- lm(log(y[tr]) ~ log(x[tr]));       yhat4 <- exp(coef(j4)[1] + coef(j4)[2]*log(x[k])); e4[k] <- y[k] - yhat4
  }
  c(Model1 = mean(e1^2), Model2 = mean(e2^2), Model3 = mean(e3^2), Model4 = mean(e4^2))
}
cat("LOO MSPE (for reference):\n")
print(round(sort(loo_mspe), 4))
```

# 1. 问题描述

对正态总体的均值 $\mu$，用 Monte Carlo 研究以下三种置信区间的**经验覆盖率**，并分别给出“左侧失误率”和“右侧失误率”（相对真实参数 $\mu$）：
1. **Standard normal bootstrap CI**：以原估计 $\hat\mu=\bar X$ 为中心，用自助法估计标准误 $\widehat{\mathrm{SE}}_{\text{boot}}$，形成
   $$
   \bar X \pm z_{1-\alpha/2}\,\widehat{\mathrm{SE}}_{\text{boot}}.
   $$
2. **Basic bootstrap CI**（又称 reverse percentile）：
   记自助估计量的经验分位数为 $q_{p}$，则
   $$
   \bigl(2\bar X - q_{1-\alpha/2},\; 2\bar X - q_{\alpha/2}\bigr).
   $$
3. **Percentile bootstrap CI**：
   $$
   \bigl(q_{\alpha/2},\; q_{1-\alpha/2}\bigr).
   $$

抽样来自正态总体（如 $N(\mu, \sigma^2)$），针对样本均值的区间估计，统计：
- 覆盖率：区间包含 $\mu$ 的比例；
- 左侧失误率：区间**在 $\mu$ 左侧**（上端点 $<\mu$）的比例；
- 右侧失误率：区间**在 $\mu$ 右侧**（下端点 $>\mu$）的比例。

# 2. 实验流程

- 固定 $(\mu,\sigma,n,\alpha)$ 与自助重复次数 $B$、Monte Carlo 重复次数 $M$；  
- 每次重复生成样本 $X_1,\dots,X_n\sim N(\mu,\sigma^2)$；  
- 以有放回重抽样法生成 $B$ 组自助样本，计算自助均值分布 $\{\bar X^*_b\}$；  
- 由三种方法形成区间并与 $\mu$ 比较；  
- 在 $M$ 次重复上计算覆盖率与左右失误率；  
- 汇总为表格。

```{r}
set.seed(2025)

## -------- parameters --------
mu     <- 0
sigma  <- 1
n      <- 25
alpha  <- 0.05
B      <- 1000
M      <- 5000
q.type <- 6

## -------- helpers --------
boot_means <- function(x, B) {
  n <- length(x)
  idx <- replicate(B, sample.int(n, n, replace = TRUE))   # n x B
  colMeans(matrix(x[idx], nrow = n, ncol = B))
}

eval_cis <- function(x) {
  xb   <- mean(x)
  bm   <- boot_means(x, B)
  se_b <- sd(bm)

  z    <- qnorm(1 - alpha/2)

  ## 1) standard normal bootstrap
  ci_norm <- c(xb - z * se_b, xb + z * se_b)

  ## 2) basic bootstrap (reverse percentile)
  qL <- quantile(bm, probs = alpha/2,     type = q.type, names = FALSE)
  qU <- quantile(bm, probs = 1 - alpha/2, type = q.type, names = FALSE)
  ci_basic <- c(2*xb - qU, 2*xb - qL)

  ## 3) percentile bootstrap
  ci_perc <- c(qL, qU)

  out <- rbind(norm = ci_norm, basic = ci_basic, perc = ci_perc)  # 3 x 2
  colnames(out) <- c("L","U")
  out
}

## 返回 3x3 矩阵（行=方法；列=coverage/miss_left/miss_right）
miss_stats <- function(CI, theta) {
  L <- CI[, "L"]; U <- CI[, "U"]
  coverage   <- (L <= theta) & (theta <= U)
  miss_left  <- (U < theta)
  miss_right <- (L > theta)
  out <- cbind(coverage, miss_left, miss_right)  # 3 x 3
  rownames(out) <- rownames(CI)
  colnames(out) <- c("coverage","miss_left","miss_right")
  out * 1  # 转为数值 0/1
}

## -------- Monte Carlo loop --------
acc <- matrix(0, nrow = 3, ncol = 3,
              dimnames = list(c("norm","basic","perc"),
                              c("coverage","miss_left","miss_right")))

for (m in 1:M) {
  x  <- rnorm(n, mean = mu, sd = sigma)
  CI <- eval_cis(x)
  ms <- miss_stats(CI, theta = mu)   # 3 x 3
  acc <- acc + ms
}

res <- acc / M
print(round(res, 4))

```


# 1. 问题

总体服从指数分布 $\mathrm{Exp}(\lambda)$（密度 $f(x)=\lambda e^{-\lambda x},\; x>0$）。样本均值记为 $\bar X$，则失效率的极大似然估计为
$$
\hat{\lambda}=\frac{1}{\bar X}.
$$

可推得
$$
\mathbb{E}(\hat{\lambda})=\lambda\frac{n}{n-1}
\quad\Rightarrow\quad
\mathrm{Bias}(\hat{\lambda})=\frac{\lambda}{n-1},
$$
以及
$$
\mathrm{SE}(\hat{\lambda})=\frac{\lambda n}{(n-1)\sqrt{\,n-2\,}}.
$$

目标：做 Monte Carlo 模拟检验 bootstrap 方法估计偏差与标准误的表现：
- 真值：$\lambda=2$；
- 样本量：$n\in\{5,10,20\}$；
- 自助重复次数：$B=1000$；
- 重复模拟：$m=1000$；
- 比较**理论偏差/SE**与**平均 bootstrap 偏差/SE**（并给出经验偏差/SE）。

# 2. 方法

对每个 $n$，重复 $m$ 次：
1. 生成样本 $X_1,\dots,X_n\stackrel{iid}{\sim}\mathrm{Exp}(\lambda)$，计算 $\hat{\lambda}=1/\bar X$；
2. 做 $B$ 次 bootstrap，得到 $\hat{\lambda}^{*(b)}=1/\bar X^{*(b)}$；
3. 计算一次实验的 bootstrap 估计
   $$
   \widehat{\mathrm{Bias}}_{\text{boot}}=\bar{\lambda}^*-\hat{\lambda},\qquad
   \widehat{\mathrm{SE}}_{\text{boot}}=\mathrm{sd}\!\left(\{\hat{\lambda}^{*(b)}\}\right);
   $$
4. 在 $m$ 次重复上取平均，得到**平均 bootstrap 偏差/SE**；并报告**经验偏差/SE**
   $$
   \widehat{\mathrm{Bias}}_{\text{emp}}=\overline{\hat{\lambda}}-\lambda,\qquad
   \widehat{\mathrm{SE}}_{\text{emp}}=\mathrm{sd}\!\left(\{\hat{\lambda}\}\right).
   $$
   
```{r}
set.seed(2026)

lambda_true <- 2
n_grid      <- c(5, 10, 20)
B           <- 1000   # bootstrap reps per dataset
m           <- 1000   # Monte Carlo datasets

## ---- helpers ----
lam_hat <- function(x) 1 / mean(x)

boot_lam <- function(x, B) {
  n <- length(x)
  idx <- replicate(B, sample.int(n, n, replace = TRUE))  # n x B
  x_star_means <- colMeans(matrix(x[idx], nrow = n, ncol = B))
  1 / x_star_means
}

## ---- main loop ----
res_list <- lapply(n_grid, function(n) {
  lam_vec   <- numeric(m)
  bias_boot <- numeric(m)
  se_boot   <- numeric(m)

  for (i in 1:m) {
    x <- rexp(n, rate = lambda_true)
    lam_hat_i <- lam_hat(x)
    lam_vec[i] <- lam_hat_i

    lam_star <- boot_lam(x, B)
    bias_boot[i] <- mean(lam_star) - lam_hat_i
    se_boot[i]   <- sd(lam_star)
  }

  ## theoretical values
  bias_theo <- lambda_true / (n - 1)
  se_theo   <- lambda_true * n / ((n - 1) * sqrt(n - 2))

  ## summaries
  data.frame(
    n = n,
    bias_theoretical = bias_theo,
    se_theoretical   = se_theo,
    bias_boot_mean   = mean(bias_boot),
    se_boot_mean     = mean(se_boot),
    bias_empirical   = mean(lam_vec) - lambda_true,
    se_empirical     = sd(lam_vec)
  )
})

summary_tbl <- do.call(rbind, res_list)
print(round(summary_tbl, 4))
```
   

# 1. 统计量（基于等式 (10.14)）

两样本 $X=\{X_1,\dots,X_n\}$ 与 $Y=\{Y_1,\dots,Y_m\}$。把两样本合并并取秩（最小为 1，最大为 $n+m$）。令
- $r_i$ 为第 $i$ 个 $X$ 在合并样本中的秩；
- $s_j$ 为第 $j$ 个 $Y$ 在合并样本中的秩。

定义
$$
U
= n\sum_{i=1}^{n}(r_i-i)^2 + m\sum_{j=1}^{m}(s_j-j)^2,
$$
则两样本 **Cramér–von Mises** 统计量为
$$
W^2=\frac{U}{nm(n+m)}-\frac{4mn-1}{6(m+n)}.
$$
$W^2$ 越大，越不支持“分布相同”的零假设。

# 2. 置换检验

在 $H_0: F=G$ 下，$W^2$ 的（近似）零分布可由**置换**获得：把合并样本随机分成大小分别为 $n$ 与 $m$ 的两组，重复 $R$ 次，得到复制统计量 $\{W^{2*}\}$，p 值近似为
$$
\hat p=\frac{1+\#\{W^{2*}\ge W^2_{\text{obs}}\}}{1+R}.
$$

# 3. 应用到示例 10.1 与 10.2 的数据

示例使用 `chickwts` 数据中 `soybean` 与 `linseed` 两组雏鸡体重，比较两组分布是否相同。


```{r}
## ---------- R code: two-sample Cramér–von Mises permutation test ----------

## CvM statistic W^2 per equation (10.14)
cvm2_stat <- function(x, y) {
  n <- length(x); m <- length(y)
  z   <- c(x, y)
  rk  <- rank(z, ties.method = "average")  # use average ranks for ties
  r   <- rk[1:n]
  s   <- rk[(n + 1):(n + m)]
  U   <- n * sum( (r - seq_len(n))^2 ) + m * sum( (s - seq_len(m))^2 )
  W2  <- U / (n * m * (n + m)) - (4 * n * m - 1) / (6 * (n + m))
  as.numeric(W2)
}

## permutation test using CvM W^2
cvm2_permute_test <- function(x, y, R = 9999, seed = 1) {
  set.seed(seed)
  n <- length(x); m <- length(y)
  z <- c(x, y)
  W2_obs <- cvm2_stat(x, y)

  reps <- numeric(R)
  for (b in seq_len(R)) {
    idx <- sample.int(n + m, n, replace = FALSE)
    reps[b] <- cvm2_stat(z[idx], z[-idx])
  }
  pval <- (1 + sum(reps >= W2_obs)) / (1 + R)
  list(stat = W2_obs, p.value = pval, reps = reps)
}

## ---------- Apply to Examples 10.1/10.2 data: chickwts (soybean vs linseed) ----------
suppressPackageStartupMessages({
  if (!requireNamespace("datasets", quietly = TRUE) ||
      !exists("chickwts", where = asNamespace("datasets"))) {
    stop("Base dataset 'chickwts' not found.")
  }
})

data(chickwts)  # in base datasets
dat <- chickwts
x <- sort(dat$weight[dat$feed == "soybean"])
y <- sort(dat$weight[dat$feed == "linseed"])

## observed W^2
W2 <- cvm2_stat(x, y)
W2

## permutation p-value
set.seed(2025)
perm_res <- cvm2_permute_test(x, y, R = 9999)
perm_res$stat
perm_res$p.value

## (optional) quick histogram of permutation null with observed W^2 line
## hist(perm_res$reps, breaks = "Scott", main = "Permutation null of W^2",
##      xlab = expression(W^2))
## abline(v = perm_res$stat, col = 2, lwd = 2)

```

---
title: "Exercise 10.7 — Permutation version of the Count Five test"
author: ""
date: "`r format(Sys.Date())`"
output:
  html_document:
    toc: true
    toc_float: true
    mathjax: default
fontsize: 12pt
---

# 1. Count Five 统计量

设两独立样本为
- $X_1,\dots,X_{n_1}$；
- $Y_1,\dots,Y_{n_2}$，

对应总体方差假设 $H_0:\sigma_1^2=\sigma_2^2$。

“Count Five” 思路是看哪一组在**总体极端区域**出现的点更多。  
记两组的最小值与最大值分别为
- $x_{(1)},x_{(n_1)}$；
- $y_{(1)},y_{(n_2)}$。

定义
- 组 $X$ 的极端点数：
  $$
  \mathrm{out}_X
    = \#\{X_i < y_{(1)}\}
      + \#\{X_i > y_{(n_2)}\},
  $$
- 组 $Y$ 的极端点数：
  $$
  \mathrm{out}_Y
    = \#\{Y_j < x_{(1)}\}
      + \#\{Y_j > x_{(n_1)}\}.
  $$

把两者中的较大者作为统计量
$$
T = \max(\mathrm{out}_X,\mathrm{out}_Y).
$$

当两组方差不等时，较大方差那一组更容易在两端出现极端值，导致 $T$ 变大。

原 Count Five 判据（样本量相等时）是“若 $T\ge4$ 则拒绝 $H_0$”，但在样本不等时该阈值不再合适，因此本题改用**置换检验**来得到 $T$ 的零分布。

# 2. 置换检验

在 $H_0:\sigma_1^2=\sigma_2^2$ 下，两组数据的标签可以置换。  
具体步骤：

1. 计算观测样本 $(x,y)$ 的统计量 $T_{\text{obs}}$；
2. 将两组数据合并为 $z=(x,y)$，长度 $N=n_1+n_2$；
3. 重复 $R$ 次：
   - 从 $\{1,\dots,N\}$ 中不放回地抽取 $n_1$ 个索引作为第一组，其余为第二组；
   - 按新的分组计算 $T^{*(b)}$；
4. 近似 $p$ 值：
   $$
   \hat p
   =\frac{1+\#\{T^{*(b)}\ge T_{\text{obs}}\}}{1+R}.
   $$

若 $\hat p\le\alpha$（例如 $0.05$）则拒绝方差相等的零假设。

该方法适用于任意 $n_1,n_2$，自动校正了样本量不等造成的影响。

# 3. 重做 Example 7.15

Example 7.15 中考察在 $N(0,1)$ 下、样本量不等时原始 Count Five 判据的经验一类错误率（发现其偏大）。  
现在用**置换 Count Five 检验**：

- 设置样本量组合，如 $(n_1,n_2)=(20,30)$、$(20,50)$；
- 在 $N(0,1)$ 下重复生成样本，按 Example 7.15 一样先减去各自的样本均值（保证均值为 0，只比较方差）；
- 对每个样本对，用上面的置换检验得到 $p$ 值，若 $p\le0.05$ 记为一次“错误拒绝”；
- 重复 $m=10000$ 次，经验一类错误率为错误拒绝的比例。

预计：与原文中固定阈值的 Count Five 相比，置换检验的一类错误率会更接近标称的 $0.05$，且对 $n_1\neq n_2$ 也适用。


```{r, echo=FALSE}
## ---------- R code for Exercise 10.7: permutation Count Five test ----------

## Count Five statistic T = max(outX, outY)
count5_stat <- function(x, y) {
  x <- as.numeric(x); y <- as.numeric(y)
  minx <- min(x); maxx <- max(x)
  miny <- min(y); maxy <- max(y)

  outX <- sum(x < miny) + sum(x > maxy)
  outY <- sum(y < minx) + sum(y > maxx)

  max(outX, outY)
}

## Permutation test based on count5_stat, one-sided (large T => unequal variances)
count5_perm_test <- function(x, y, R = 1999, seed = NULL) {
  if (!is.null(seed)) set.seed(seed)
  n1 <- length(x); n2 <- length(y)
  z  <- c(x, y)

  T_obs <- count5_stat(x, y)
  reps  <- numeric(R)

  for (b in seq_len(R)) {
    idx1 <- sample.int(n1 + n2, n1, replace = FALSE)
    x1   <- z[idx1]
    y1   <- z[-idx1]
    reps[b] <- count5_stat(x1, y1)
  }

  pval <- (1 + sum(reps >= T_obs)) / (1 + R)
  list(stat = T_obs, p.value = pval, reps = reps)
}

## ---------- Repeat Example 7.15 with permutation test ----------

set.seed(2027)

one_setting <- function(n1, n2, m = 10000, R = 999) {
  mu1 <- mu2 <- 0
  sigma1 <- sigma2 <- 1

  rej <- logical(m)

  for (i in seq_len(m)) {
    x <- rnorm(n1, mu1, sigma1)
    y <- rnorm(n2, mu2, sigma2)
    ## center by sample mean (as in Example 7.15)
    x <- x - mean(x)
    y <- y - mean(y)

    p <- count5_perm_test(x, y, R = R)$p.value
    rej[i] <- (p <= 0.05)
  }
  mean(rej)
}

## empirical Type I error for n1 = 20, n2 = 30
alpha_20_30 <- one_setting(20, 30, m = 2000, R = 999)  # m=2000 for speed; increase if desired
alpha_20_30

## empirical Type I error for n1 = 20, n2 = 50
alpha_20_50 <- one_setting(20, 50, m = 2000, R = 999)
alpha_20_50

c(alpha_20_30 = alpha_20_30, alpha_20_50 = alpha_20_50)

```

---
title: "Proof of Stationary Distribution for Metropolis-Hastings (Continuous Case)"
output:
  html_document:
    mathjax: default
---

# Problem

Prove that the stationary distribution of the Metropolis–Hastings sampler with proposal distribution \( g(y \mid x) \) is the target distribution \( f(x) \) in the continuous case.

# 1. Metropolis–Hastings Transition Kernel

Let the current state be \( x \), the target density be \( f(x) \), and the proposal density be \( g(y \mid x) \).

The acceptance probability is

\[
\alpha(x,y) = \min\left\{1,\,
\frac{f(y)\,g(x\mid y)}{f(x)\,g(y\mid x)}
\right\}.
\]

If \( y \neq x \), the transition kernel density is

\[
K(x,y) = g(y \mid x)\,\alpha(x,y).
\]

The probability of staying at \( x \) is

\[
r(x) = 1 - \int g(y \mid x)\,\alpha(x,y)\,dy.
\]

To prove stationarity of \( f \), it suffices to prove the detailed balance condition:

\[
f(x)K(x,y) = f(y)K(y,x), \qquad x \neq y.
\]

If detailed balance holds, then \( f \) is a stationary distribution of the Markov chain.

# 2. Verification of Detailed Balance

For \( x \neq y \),

\[
K(x,y) = g(y \mid x)\,\alpha(x,y), 
\qquad 
K(y,x) = g(x \mid y)\,\alpha(y,x).
\]

We need to show:

\[
f(x)\,g(y\mid x)\,\alpha(x,y)
=
f(y)\,g(x\mid y)\,\alpha(y,x).
\]

Consider two cases.

---

## Case 1: Ratio ≤ 1

\[
\frac{f(y)\,g(x\mid y)}{f(x)\,g(y\mid x)} \le 1.
\]

Thus:

\[
\alpha(x,y)
=
\frac{f(y)\,g(x\mid y)}{f(x)\,g(y\mid x)}, 
\qquad 
\alpha(y,x)=1.
\]

Then:

\[
\begin{aligned}
f(x)\,g(y\mid x)\,\alpha(x,y)
&= f(x)\,g(y\mid x)
   \frac{f(y)\,g(x\mid y)}{f(x)\,g(y\mid x)}   \\[6pt]
&= f(y)\,g(x\mid y)                        \\[6pt]
&= f(y)\,g(x\mid y)\,\alpha(y,x)            \\[6pt]
&= f(y)\,K(y,x).
\end{aligned}
\]

---

## Case 2: Ratio > 1

\[
\frac{f(y)\,g(x\mid y)}{f(x)\,g(y\mid x)} > 1.
\]

Then:

\[
\alpha(x,y) = 1, 
\qquad 
\alpha(y,x)=
\frac{f(x)\,g(y\mid x)}{f(y)\,g(x\mid y)}.
\]

Thus:

\[
\begin{aligned}
f(x)\,g(y\mid x)\,\alpha(x,y)
&= f(x)\,g(y\mid x)                              \\[6pt]
&= f(y)\,g(x\mid y)
   \frac{f(x)\,g(y\mid x)}{f(y)\,g(x\mid y)}      \\[6pt]
&= f(y)\,g(x\mid y)\,\alpha(y,x)                 \\[6pt]
&= f(y)\,K(y,x).
\end{aligned}
\]

---

Thus for all \( x\neq y \):

\[
f(x)\,K(x,y) = f(y)\,K(y,x).
\]

# 3. Stationarity of \( f \)

For any measurable set \( A \),

\[
\int f(x)\,K(x,y)\,dx = f(y).
\]

Thus the distribution does not change after one step:

\[
\int f(x)\,K(x,y)\,dx = f(y),
\]

and therefore \( f \) is the stationary distribution of the Metropolis–Hastings chain.


# Problem

We want to implement a random–walk Metropolis sampler for generating observations from the **standard Laplace distribution**.

The standard Laplace density (location \(0\), scale \(1\)) is

$$
f(x) = \frac{1}{2}\exp\big(-|x|\big), \quad x \in \mathbb{R}.
$$

We use a **normal random walk proposal**: given the current state \(X_t = x\), we propose

$$
Y \sim N(x,\sigma^2),
$$

where \(\sigma^2\) is the variance of the proposal distribution.

Tasks:

1. Implement the Metropolis algorithm for this random–walk with Laplace target.
2. Run the chain for several different proposal variances \(\sigma^2\) (for example, \(\sigma^2 = 0.1^2, 1^2, 5^2\)).
3. Compare the resulting chains (trace plots, histograms, autocorrelation).
4. Compute and report the **acceptance rate** for each chain.

# Method

Because the proposal is **symmetric**,

$$
g(y \mid x) = g(x \mid y) \quad \text{for all } x,y,
$$

the Metropolis acceptance probability simplifies to

$$
\alpha(x,y)
= \min\left\{1,\,
\frac{f(y)}{f(x)}
\right\}
= \min\left\{1,\,
\exp\big(-|y| + |x|\big)
\right\}.
$$

Algorithm for a chain of length \(N\):

1. Choose an initial value \(X_0\) (for example \(X_0 = 0\)) and proposal standard deviation \(\sigma\).
2. For \(t = 0,1,\dots,N-1\):

   1. Propose \(Y \sim N(X_t,\sigma^2)\).
   2. Compute the log acceptance ratio
      $$
      \log \alpha = -|Y| + |X_t|.
      $$
   3. Draw \(U \sim \text{Unif}(0,1)\).  
      If \(\log U < \log \alpha\), accept the proposal and set \(X_{t+1} = Y\);  
      otherwise, reject and set \(X_{t+1} = X_t\).

3. After discarding an initial burn–in part, treat the remaining \(X_t\) as approximate samples from the Laplace distribution.

The **acceptance rate** is

$$
\text{acc\_rate} =
\frac{\text{number of accepted proposals}}{\text{number of iterations}}.
$$

Qualitative expectations:

- If \(\sigma^2\) is too small, the chain moves slowly (high acceptance, strong autocorrelation).
- If \(\sigma^2\) is too large, most proposals fall in low–density regions (very low acceptance, chain sticks).
- A moderate \(\sigma\) gives a reasonable acceptance rate (often around \(0.3\)–\(0.6\) in 1D) and good mixing.

We will implement an R function that simulates the chain for a given \(\sigma\), reports the acceptance rate, and compare several choices of \(\sigma\).

```{r}
## Random-walk Metropolis sampler for standard Laplace(0, 1)
## target density f(x) = 0.5 * exp(-abs(x))

# log target density (up to an additive constant)
log_f_laplace <- function(x) {
  -abs(x) - log(2)  # log(1/2 * exp(-|x|))
}

# random-walk Metropolis sampler
rw_metropolis_laplace <- function(N, sigma, x0 = 0) {
  x <- numeric(N)
  x[1] <- x0
  n_accept <- 0L
  
  for (t in 2:N) {
    # propose from N(current, sigma^2)
    y <- rnorm(1, mean = x[t - 1], sd = sigma)
    
    # symmetric proposal -> only target ratio appears
    log_alpha <- log_f_laplace(y) - log_f_laplace(x[t - 1])
    
    if (log(runif(1)) < log_alpha) {
      x[t] <- y
      n_accept <- n_accept + 1L
    } else {
      x[t] <- x[t - 1]
    }
  }
  
  acc_rate <- n_accept / (N - 1)
  list(chain = x, acc_rate = acc_rate)
}

## ---- run chains with different proposal variances ----
set.seed(2025)

N      <- 10000    # total iterations
burnin <- 1000     # burn-in length

sigmas <- c(0.1, 1, 5)  # proposal standard deviations to compare
chains <- lapply(sigmas, function(s) rw_metropolis_laplace(N, sigma = s))

# acceptance rates
acc_rates <- sapply(chains, function(res) res$acc_rate)
names(acc_rates) <- paste0("sigma=", sigmas)
acc_rates

## ---- simple comparison plots ----
par(mfrow = c(3, 2))

for (i in seq_along(sigmas)) {
  s   <- sigmas[i]
  ch  <- chains[[i]]$chain
  ch2 <- ch[(burnin + 1):N]
  
  # trace plot
  plot(ch2, type = "l",
       main = paste("Trace plot, sigma =", s),
       xlab = "Iteration (post burn-in)", ylab = "X")
  
  # histogram vs true Laplace density
  hist(ch2, freq = FALSE, breaks = "FD",
       main = paste("Histogram, sigma =", s),
       xlab = "X")
  xs <- seq(min(ch2), max(ch2), length.out = 200)
  lines(xs, 0.5 * exp(-abs(xs)), lwd = 2, lty = 2)  # true Laplace pdf
}

par(mfrow = c(1, 1))

## ---- optional: autocorrelation comparison ----
par(mfrow = c(3, 1))
for (i in seq_along(sigmas)) {
  ch2 <- chains[[i]]$chain[(burnin + 1):N]
  acf(ch2, lag.max = 50, main = paste("ACF, sigma =", sigmas[i]))
}
par(mfrow = c(1, 1))

```


## Problem

Consider the bivariate density

$$
f(x,y) \propto \binom{n}{x} y^{x+a-1}(1-y)^{n-x+b-1},
\quad x = 0,1,\dots,n,\; 0 \le y \le 1,
$$

where $a>0$, $b>0$, $n$ is a positive integer.

It can be shown that, for fixed $a,b,n$, the full conditional distributions are

- $X \mid Y=y \sim \mathrm{Binomial}(n, y)$,
- $Y \mid X=x \sim \mathrm{Beta}(x+a, n-x+b)$.

Use the **Gibbs sampler** to generate a Markov chain whose stationary distribution has joint density $f(x,y)$.

## Method

From the joint density

$$
f(x,y) \propto \binom{n}{x} y^{x+a-1}(1-y)^{n-x+b-1},
$$

we can derive the full conditionals:

1. **Conditional of $X$ given $Y=y$**

Treat terms that do not depend on $x$ as a constant. Up to proportionality,

$$
f(x \mid y)
\propto \binom{n}{x} y^x (1-y)^{n-x},
\quad x=0,1,\dots,n,
$$

which is exactly the pmf of $\mathrm{Binomial}(n,y)$.  
Hence

$$
X \mid Y=y \sim \mathrm{Binomial}(n, y).
$$

2. **Conditional of $Y$ given $X=x$**

Treat $\binom{n}{x}$ as a constant in $y$. Up to proportionality,

$$
f(y \mid x)
\propto y^{x+a-1}(1-y)^{n-x+b-1}, \quad 0<y<1,
$$

which is the pdf of $\mathrm{Beta}(x+a, n-x+b)$.  
Hence

$$
Y \mid X=x \sim \mathrm{Beta}(x+a, n-x+b).
$$

Therefore a Gibbs sampler is very simple:

- At iteration $t$, given current state $(X_t,Y_t)$,
  1. Sample
     $$
     X_{t+1} \sim \mathrm{Binomial}\bigl(n,\,Y_t\bigr).
     $$
  2. Then sample
     $$
     Y_{t+1} \sim \mathrm{Beta}\bigl(X_{t+1}+a,\;n-X_{t+1}+b\bigr).
     $$

This defines a Markov chain $\{(X_t,Y_t)\}_{t\ge 0}$ whose stationary distribution is the target joint density $f(x,y)$.

In practice:

- Choose specific values for $a,b,n$ (for example $a=2,\ b=2,\ n=10$);
- Choose an initial state $(X_0,Y_0)$, e.g. $X_0 = 0,\ Y_0 = 0.5$;
- Run the Gibbs sampler for a large number of iterations $N$;
- Discard an initial burn-in (e.g. first 1000 iterates);
- Use the remaining samples to:

  - Check empirical marginal distributions of $X$ and $Y$;
  - Draw trace plots and scatter plots for $(X_t,Y_t)$.

Because both full conditionals are standard distributions (Binomial and Beta), implementation in R is straightforward using `rbinom()` and `rbeta()`.


```{r}
## Gibbs sampler for the bivariate density in Exercise 11.10
## f(x,y) ∝ choose(n, x) * y^(x+a-1) * (1-y)^(n-x+b-1)
## with X | Y=y ~ Binomial(n, y), Y | X=x ~ Beta(x+a, n-x+b)

gibbs_biv <- function(N, a = 2, b = 2, n = 10,
                      x0 = 0L, y0 = 0.5) {
  x <- integer(N)
  y <- numeric(N)
  x[1] <- x0
  y[1] <- y0
  
  for (t in 2:N) {
    ## X | Y ~ Binomial(n, y)
    x[t] <- rbinom(1, size = n, prob = y[t - 1])
    ## Y | X ~ Beta(x + a, n - x + b)
    y[t] <- rbeta(1, shape1 = x[t] + a, shape2 = n - x[t] + b)
  }
  
  list(x = x, y = y)
}

## ---- run the Gibbs sampler ----
set.seed(2025)

N      <- 10000
burnin <- 1000

out <- gibbs_biv(N, a = 2, b = 2, n = 10,
                 x0 = 0L, y0 = 0.5)

x_chain <- out$x[(burnin + 1):N]
y_chain <- out$y[(burnin + 1):N]

## ---- quick diagnostics ----
par(mfrow = c(2, 2))

plot(x_chain, type = "l",
     main = "Trace of X", xlab = "Iteration", ylab = "X")
plot(y_chain, type = "l",
     main = "Trace of Y", xlab = "Iteration", ylab = "Y")

hist(x_chain, breaks = seq(-0.5, 10.5, by = 1), freq = FALSE,
     main = "Empirical marginal of X")
hist(y_chain, breaks = "FD", freq = FALSE,
     main = "Empirical marginal of Y")

par(mfrow = c(1, 1))

## Scatter plot of joint draws
plot(x_chain, y_chain, pch = 20, cex = 0.5,
     xlab = "X", ylab = "Y", main = "Joint draws (Gibbs)")

```

### Gelman–Rubin 统计量

假设我们有 $m$ 条链，每条链在丢弃 burn-in 后保留 $n$ 个样本，记第 $i$ 条链的第 $j$ 个样本为 $\theta_{ij}$。

链内均值与整体均值：
$$
\bar\theta_i = \frac{1}{n}\sum_{j=1}^n \theta_{ij}, \qquad
\bar\theta = \frac{1}{m}\sum_{i=1}^m \bar\theta_i.
$$

链间方差：
$$
B = \frac{n}{m-1}\sum_{i=1}^m \left(\bar\theta_i - \bar\theta\right)^2.
$$

链内方差：
$$
W = \frac{1}{m}\sum_{i=1}^m
\left[
\frac{1}{n-1}\sum_{j=1}^n
\left(\theta_{ij} - \bar\theta_i\right)^2
\right].
$$

方差估计：
$$
\hat V = \frac{n-1}{n} W + \frac{1}{n} B.
$$

Gelman–Rubin 统计量：
$$
\hat R = \sqrt{\frac{\hat V}{W}}.
$$

一般经验：若 $\hat R < 1.2$（最好接近 $1$），就认为链差不多收敛到目标分布。

```{r}
## -----------------------------------------------------------
##  Common tools
## -----------------------------------------------------------
if (!requireNamespace("coda", quietly = TRUE)) {
  install.packages("coda")
}
library(coda)

## 辅助函数：给一组链（矩阵，每列一条链）算 Gelman-Rubin 诊断
gelman_for_matrix <- function(mat) {
  # mat: n x m, n iterations, m chains
  mcmc_list <- mcmc.list(
    lapply(seq_len(ncol(mat)), function(j) coda::mcmc(mat[, j]))
  )
  coda::gelman.diag(mcmc_list)
}

## -----------------------------------------------------------
##  Exercise 11.6: random-walk Metropolis for Laplace(0,1)
## -----------------------------------------------------------

log_f_laplace <- function(x) {
  -abs(x) - log(2)  # log density of Laplace(0,1) up to normalizing constant
}

rw_laplace_single_chain <- function(N, sigma, x0) {
  x <- numeric(N)
  x[1] <- x0
  for (t in 2:N) {
    y <- rnorm(1, mean = x[t - 1], sd = sigma)
    log_alpha <- log_f_laplace(y) - log_f_laplace(x[t - 1])
    if (log(runif(1)) < log_alpha) {
      x[t] <- y
    } else {
      x[t] <- x[t - 1]
    }
  }
  x
}

## 参数
set.seed(2025)
N      <- 20000         # 总迭代
burnin <- 5000          # burn-in
n_keep <- N - burnin
sigma  <- 1             # 任选一个 proposal 标准差（可以改成 0.1, 5 等）

n_chains <- 4
inits    <- c(-5, 0, 5, 10)  # 过度分散的起点

## 生成多条链
laplace_chains <- sapply(seq_len(n_chains), function(j) {
  rw_laplace_single_chain(N = N, sigma = sigma, x0 = inits[j])
})

## 丢弃 burn-in
laplace_post <- laplace_chains[(burnin + 1):N, ]  # n_keep x n_chains

## Gelman-Rubin 诊断
cat("Exercise 11.6, sigma =", sigma, "\n")
gr_laplace <- gelman_for_matrix(laplace_post)
print(gr_laplace)

## -----------------------------------------------------------
##  Exercise 11.10: Gibbs sampler for bivariate (X, Y)
##      f(x,y) ∝ C(n, x) y^{x+a-1}(1-y)^{n-x+b-1}
##      X | Y=y ~ Binomial(n, y)
##      Y | X=x ~ Beta(x+a, n-x+b)
## -----------------------------------------------------------

gibbs_biv_single_chain <- function(N, a, b, n, x0, y0) {
  x <- integer(N)
  y <- numeric(N)
  x[1] <- x0
  y[1] <- y0
  for (t in 2:N) {
    x[t] <- rbinom(1, size = n, prob = y[t - 1])
    y[t] <- rbeta(1, shape1 = x[t] + a, shape2 = n - x[t] + b)
  }
  list(x = x, y = y)
}

## 参数
set.seed(2026)
N      <- 20000
burnin <- 5000
n_keep <- N - burnin

a <- 2
b <- 2
n <- 10

n_chains <- 4
init_x   <- c(0L, 5L, 10L, 2L)
init_y   <- c(0.1, 0.9, 0.5, 0.2)

## 生成多条链
x_mat <- matrix(NA_integer_, nrow = N, ncol = n_chains)
y_mat <- matrix(NA_real_,     nrow = N, ncol = n_chains)

for (j in seq_len(n_chains)) {
  out <- gibbs_biv_single_chain(N, a, b, n,
                                x0 = init_x[j], y0 = init_y[j])
  x_mat[, j] <- out$x
  y_mat[, j] <- out$y
}

## 丢弃 burn-in
x_post <- x_mat[(burnin + 1):N, ]
y_post <- y_mat[(burnin + 1):N, ]

cat("\nExercise 11.10: Gelman-Rubin for X\n")
gr_x <- gelman_for_matrix(x_post)
print(gr_x)

cat("\nExercise 11.10: Gelman-Rubin for Y\n")
gr_y <- gelman_for_matrix(y_post)
print(gr_y)

## 如果想要多变量的 \hat{R}，可以把 (X, Y) 组合起来：
cat("\nExercise 11.10: multivariate Gelman-Rubin for (X, Y)\n")
mcmc_list_xy <- mcmc.list(
  lapply(seq_len(n_chains), function(j) {
    coda::mcmc(cbind(x_post[, j], y_post[, j]))
  })
)
gr_xy <- coda::gelman.diag(mcmc_list_xy, multivariate = TRUE)
print(gr_xy)

```

## Problem

Implement a combination of `Map()` and `vapply()` to create an `lapply()` variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?

## Idea

- `Map(FUN, ...)` (or `mapply(FUN, ..., SIMPLIFY = FALSE)`) can iterate **in parallel** over multiple input vectors/lists and returns a list.
- `vapply(X, FUN, FUN.VALUE, ...)` is a type-stable version of `sapply()` that forces the output to match `FUN.VALUE` (vector or matrix).

So the plan is:

1. Use `Map()` to produce a list of results (one result per “parallel step”).
2. Use `vapply()` on that list to coerce results into a vector or a matrix with a guaranteed type/shape.

## What arguments should the function take?

A good, general signature is:

- `FUN`: the function to apply.
- `...`: multiple inputs to iterate over in parallel (like `Map()` / `mapply()`).
- `FUN.VALUE`: a template specifying the type and length/dim of each result (required by `vapply()`).
- `USE.NAMES`: whether to keep names (optional).
- `...` for extra arguments to `FUN` (either included via closure or a separate argument; the simplest is to allow extra arguments after `FUN.VALUE`).

A practical base-R design is:

- `p_lapply <- function(FUN, ..., FUN.VALUE, USE.NAMES = TRUE)`
  where `FUN` is applied to each “zipped” set of inputs.
  
```{r}
## ------------------------------------------------------------
## Parallel lapply using Map() + vapply()
## ------------------------------------------------------------

p_lapply <- function(FUN, ..., FUN.VALUE, USE.NAMES = TRUE) {
  # Step 1: iterate in parallel over all inputs
  res_list <- Map(FUN, ...)
  
  # Step 2: enforce output type/shape using vapply
  vapply(res_list,
         FUN = identity,
         FUN.VALUE = FUN.VALUE,
         USE.NAMES = USE.NAMES)
}

## ------------------------------------------------------------
## Example 1: vector output
## Each call returns a single numeric value
## ------------------------------------------------------------

result_vec <- p_lapply(
  function(a, b) a + b,
  1:5,
  11:15,
  FUN.VALUE = numeric(1)
)

cat("Example 1: vector output\n")
print(result_vec)

## ------------------------------------------------------------
## Example 2: matrix output
## Each call returns a numeric vector of length 2
## ------------------------------------------------------------

result_mat <- p_lapply(
  function(a, b) c(a, b),
  1:3,
  11:13,
  FUN.VALUE = numeric(2)
)

cat("\nExample 2: matrix output\n")
print(result_mat)

## ------------------------------------------------------------
## Example 3: non-numeric output (character vector)
## ------------------------------------------------------------

result_char <- p_lapply(
  function(a, b) paste(a, b, sep = "-"),
  letters[1:4],
  LETTERS[1:4],
  FUN.VALUE = character(1)
)

cat("\nExample 3: character vector output\n")
print(result_char)

```

## Problem

We are asked to implement a faster version of `chisq.test()` that computes **only the chi-square test statistic** when the input consists of two numeric vectors $x$ and $y$ with no missing values.

Unlike the general-purpose `chisq.test()` function in R, we do not need to handle missing values, contingency-table inputs, continuity corrections, or p-value calculations. Our goal is simply to compute the Pearson chi-square statistic efficiently.

## Mathematical definition

Given two categorical variables represented by numeric vectors $x$ and $y$, we first form a contingency table with observed cell counts $O_{ij}$.

Let $r_i$ denote the sum of row $i$, $c_j$ denote the sum of column $j$, and let $n$ be the total sample size. The expected cell counts are given by

$$
E_{ij} = \frac{r_i c_j}{n}.
$$

The Pearson chi-square statistic is defined as

$$
\chi^2 = \sum_{i,j} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}.
$$

## Strategy for a faster implementation

To obtain a faster version of `chisq.test()`, we exploit the fact that the inputs are already valid numeric vectors with no missing values. The procedure is as follows:

1. Construct the contingency table using `table()`.
2. Compute the row and column marginal sums using `rowSums()` and `colSums()`.
3. Compute the expected counts via an outer product.
4. Evaluate the chi-square statistic using vectorized arithmetic.

By avoiding unnecessary input checks and additional computations, this implementation is substantially faster than the general `chisq.test()` function.

```{r}
## ------------------------------------------------------------
## Fast computation of Pearson chi-square test statistic
## ------------------------------------------------------------

fast_chisq_stat <- function(x, y) {
  # x, y: numeric vectors with no missing values
  
  # observed counts
  O <- table(x, y)
  
  # marginal sums
  rs <- rowSums(O)
  cs <- colSums(O)
  n  <- sum(O)
  
  # expected counts
  E <- outer(rs, cs) / n
  
  # Pearson chi-square statistic
  sum((O - E)^2 / E)
}

## ------------------------------------------------------------
## Example: compare with chisq.test()
## ------------------------------------------------------------

set.seed(123)

# generate two categorical numeric vectors
x <- sample(1:4, 1000, replace = TRUE)
y <- sample(1:3, 1000, replace = TRUE)

# fast version
chi_fast <- fast_chisq_stat(x, y)

# base R chisq.test (without Yates correction)
chi_base <- chisq.test(x, y, correct = FALSE)$statistic

cat("Fast chi-square statistic:\n")
print(chi_fast)

cat("\nchisq.test() statistic:\n")
print(chi_base)

```

## Problem

We want a faster version of `table()` for the special case where the input is two integer vectors $x$ and $y$ with no missing values. The goal is to compute the contingency table (joint frequency counts) more efficiently than the general `table()` implementation.

We then use this faster 2D table builder to speed up the chi-square statistic computation, since the chi-square statistic depends only on the observed counts table $O_{ij}$ and the expected counts $E_{ij}$.

## Key idea

For two integer vectors $x$ and $y$ of equal length $n$, define the observed count table $O$ where
$O_{ij}$ equals the number of indices $k$ such that $x_k = i$ and $y_k = j$.

A fast approach is:

1. Map each pair $(x_k, y_k)$ to a single integer bin index.
2. Use `tabulate()` (which is implemented in optimized C) to count occurrences of those bin indices.
3. Reshape the resulting vector of counts into a matrix.

If we know that $x \in \{1,\dots,K\}$ and $y \in \{1,\dots,L\}$, we can encode
each pair by

$$
b_k = x_k + (y_k - 1)K,
$$

so $b_k \in \{1,\dots, KL\}$. Then the contingency table is obtained by

$$
O = \mathrm{matrix}\big(\mathrm{tabulate}(b,\, nbins=KL),\, nrow=K,\, ncol=L\big).
$$

## Using it to speed up the chi-square statistic

The Pearson chi-square statistic for testing independence is

$$
\chi^2 = \sum_{i=1}^K \sum_{j=1}^L \frac{(O_{ij} - E_{ij})^2}{E_{ij}},
\qquad
E_{ij} = \frac{r_i c_j}{n},
$$

where $r_i$ and $c_j$ are row and column totals of $O$.

If we compute $O$ faster than `table()`, we can compute $\chi^2$ faster as well, especially in simulation settings where the test is repeated many times.

```{r}
## ------------------------------------------------------------
## Fast 2D table for two integer vectors (no NA)
## Assumptions:
##   - x, y are integer (or numeric but representing integers)
##   - x in 1..K, y in 1..L (or we can map them to 1..K/L)
## ------------------------------------------------------------

fast_table2_int <- function(x, y, K = NULL, L = NULL) {
  # x, y: integer vectors of equal length, no NA
  # K, L: max levels for x and y (optional; inferred if NULL)
  stopifnot(length(x) == length(y))

  x <- as.integer(x)
  y <- as.integer(y)

  if (is.null(K)) K <- max(x)
  if (is.null(L)) L <- max(y)

  # Bin encoding: b = x + (y-1)*K  in 1..K*L
  b <- x + (y - 1L) * K
  counts <- tabulate(b, nbins = K * L)

  matrix(counts, nrow = K, ncol = L)
}

## ------------------------------------------------------------
## Fast chi-square statistic using fast_table2_int()
## (no correction, statistic only)
## ------------------------------------------------------------

fast_chisq_stat_int <- function(x, y, K = NULL, L = NULL) {
  O <- fast_table2_int(x, y, K = K, L = L)
  rs <- rowSums(O)
  cs <- colSums(O)
  n  <- sum(O)
  E  <- outer(rs, cs) / n
  sum((O - E)^2 / E)
}

## ------------------------------------------------------------
## Examples (not commented): verify correctness vs base R
## ------------------------------------------------------------

set.seed(123)

# Example data: two integer categorical vectors, no NA
x <- sample(1:4, 10000, replace = TRUE)
y <- sample(1:3, 10000, replace = TRUE)

cat("Example: fast_table2_int vs table()\n")
O_fast <- fast_table2_int(x, y)
O_base <- table(x, y)

print(O_fast)
cat("\nIdentical to table()? -> ", identical(unname(O_fast), unname(O_base)), "\n", sep = "")

cat("\nExample: chi-square statistic (fast vs chisq.test)\n")
chi_fast <- fast_chisq_stat_int(x, y)
chi_base <- chisq.test(x, y, correct = FALSE)$statistic

cat("fast chi-square:", chi_fast, "\n")
cat("chisq.test stat:", as.numeric(chi_base), "\n")

## ------------------------------------------------------------
## Optional: quick timing (base R only)
## ------------------------------------------------------------
cat("\nTiming (system.time) over 200 repetitions:\n")

t1 <- system.time({
  for (i in 1:200) table(x, y)
})

t2 <- system.time({
  for (i in 1:200) fast_table2_int(x, y)
})

t3 <- system.time({
  for (i in 1:200) chisq.test(x, y, correct = FALSE)$statistic
})

t4 <- system.time({
  for (i in 1:200) fast_chisq_stat_int(x, y)
})

cat("table():\n"); print(t1)
cat("fast_table2_int():\n"); print(t2)
cat("chisq.test() stat only:\n"); print(t3)
cat("fast_chisq_stat_int():\n"); print(t4)

```

## Problem description

We consider an example due to Rao on genetic linkage data. The observed sample consists of $n = 197$ animals classified into four categories, with observed group sizes

$$
(x_1, x_2, x_3, x_4) = (125, 18, 20, 34).
$$

Assume that the data follow a multinomial distribution with category probabilities depending on a scalar parameter $\theta \in (0,1)$:

$$
(p_1(\theta), p_2(\theta), p_3(\theta), p_4(\theta))
=
\left(
\frac{1}{2} + \frac{\theta}{4},
\frac{1-\theta}{4},
\frac{1-\theta}{4},
\frac{\theta}{4}
\right).
$$

The goal is to estimate the **posterior distribution of $\theta$ given the observed data**, using one of the computational methods discussed in this chapter.

## Likelihood function

Given $\theta$, the joint distribution of the counts $(x_1, x_2, x_3, x_4)$ is multinomial with total count $n$ and probabilities $p_i(\theta)$. The likelihood function is therefore

$$
L(\theta \mid x)
\propto
\prod_{i=1}^4 p_i(\theta)^{x_i}.
$$

Substituting the specific form of the probabilities yields

$$
L(\theta \mid x)
\propto
\left(\frac{1}{2} + \frac{\theta}{4}\right)^{x_1}
\left(\frac{1-\theta}{4}\right)^{x_2 + x_3}
\left(\frac{\theta}{4}\right)^{x_4}.
$$

This likelihood is supported on the interval $\theta \in (0,1)$ and is not conjugate to any standard prior distribution.

## Prior specification

To complete the Bayesian model, we assign a prior distribution to $\theta$. A natural and simple choice is the uniform prior on $(0,1)$,

$$
\theta \sim \mathrm{Beta}(1,1),
$$

although other Beta priors $\mathrm{Beta}(a,b)$ could also be used to incorporate prior information.

## Posterior distribution

By Bayes’ theorem, the posterior density of $\theta$ given the data is

$$
\pi(\theta \mid x)
\propto
L(\theta \mid x)\,\pi(\theta),
\qquad 0 < \theta < 1.
$$

Because the posterior density does not have a closed-form expression that can be easily normalized or sampled from directly, we must resort to **numerical methods**.

## Computational approach

Since the parameter space is one-dimensional, an effective and reliable approach is **grid-based approximation**:

1. Discretize the interval $(0,1)$ into a fine grid of $\theta$ values.
2. Evaluate the unnormalized log-posterior at each grid point.
3. Exponentiate and normalize the values to obtain an approximate posterior density on the grid.
4. Use this discrete approximation to compute posterior summaries or to draw samples.

This approach avoids tuning issues associated with Markov chain Monte Carlo methods and provides an accurate approximation when the grid is sufficiently fine.

The resulting approximation yields estimates of posterior quantities such as the posterior mean, posterior credible intervals, and the maximum a posteriori (MAP) estimate of $\theta$.

```{r}
library(Rcpp)

Rcpp::cppFunction(code = '
#include <Rcpp.h>
using namespace Rcpp;

// log-sum-exp for numerical stability
double log_sum_exp(const NumericVector& x) {
  double m = x[0];
  for (int i = 1; i < x.size(); ++i) if (x[i] > m) m = x[i];
  double s = 0.0;
  for (int i = 0; i < x.size(); ++i) s += std::exp(x[i] - m);
  return m + std::log(s);
}

// [[Rcpp::export]]
Rcpp::List posterior_theta_grid_cpp(Rcpp::IntegerVector counts,
                                    int grid_n = 20001,
                                    int n_samp = 5000,
                                    double prior_a = 1.0,
                                    double prior_b = 1.0,
                                    double eps = 1e-12) {
  if (counts.size() != 4) stop("counts must have length 4.");
  if (grid_n < 3) stop("grid_n must be >= 3.");
  if (n_samp < 1) stop("n_samp must be >= 1.");
  if (prior_a <= 0.0 || prior_b <= 0.0) stop("prior_a, prior_b must be > 0.");
  if (eps <= 0.0 || eps >= 1e-3) stop("eps should be small, e.g. 1e-12.");

  int x1 = counts[0], x2 = counts[1], x3 = counts[2], x4 = counts[3];

  // grid on (0,1), avoid endpoints for log terms
  NumericVector theta(grid_n);
  NumericVector logpost(grid_n);

  double step = (1.0 - 2.0*eps) / (grid_n - 1);
  for (int i = 0; i < grid_n; ++i) {
    double th = eps + i * step;
    theta[i] = th;

    // probabilities
    double p1 = 0.5 + th/4.0;
    double p2 = (1.0 - th)/4.0;
    double p3 = (1.0 - th)/4.0;
    double p4 = th/4.0;

    // log-likelihood up to additive constant (multinomial coefficient ignored)
    double ll = 0.0;
    ll += x1 * std::log(p1);
    ll += x2 * std::log(p2);
    ll += x3 * std::log(p3);
    ll += x4 * std::log(p4);

    // Beta(prior_a, prior_b) prior on theta
    double lp = (prior_a - 1.0) * std::log(th) + (prior_b - 1.0) * std::log(1.0 - th);

    logpost[i] = ll + lp;
  }

  // normalize on grid: posterior mass proportional to exp(logpost)
  double lse = log_sum_exp(logpost);
  NumericVector w(grid_n);
  for (int i = 0; i < grid_n; ++i) w[i] = std::exp(logpost[i] - lse);

  // build CDF for sampling from discrete approximation
  NumericVector cdf(grid_n);
  double cum = 0.0;
  for (int i = 0; i < grid_n; ++i) {
    cum += w[i];
    cdf[i] = cum;
  }
  // ensure last is exactly 1 (avoid floating drift)
  cdf[grid_n - 1] = 1.0;

  // sample indices by inverse CDF
  NumericVector samp(n_samp);
  for (int s = 0; s < n_samp; ++s) {
    double u = R::runif(0.0, 1.0);
    // binary search
    int lo = 0, hi = grid_n - 1;
    while (lo < hi) {
      int mid = lo + (hi - lo) / 2;
      if (cdf[mid] >= u) hi = mid;
      else lo = mid + 1;
    }
    samp[s] = theta[lo];
  }

  return List::create(
    _["theta_grid"] = theta,
    _["log_posterior_unnorm"] = logpost,
    _["posterior_mass_grid"] = w,
    _["cdf_grid"] = cdf,
    _["samples_theta"] = samp
  );
}
')

## -------------------- Example: Rao data --------------------
counts <- c(125L, 18L, 20L, 34L)

res <- posterior_theta_grid_cpp(
  counts   = counts,
  grid_n   = 20001,
  n_samp   = 10000,
  prior_a  = 1,     # Uniform(0,1)
  prior_b  = 1
)

## posterior summaries from samples
theta_s <- res$samples_theta
cat("Posterior mean:", mean(theta_s), "\n")
cat("Posterior sd  :", sd(theta_s), "\n")
cat("Posterior 2.5%, 50%, 97.5% quantiles:\n")
print(quantile(theta_s, c(0.025, 0.5, 0.975)))

## MAP estimate from grid (argmax log posterior)
theta_map <- res$theta_grid[ which.max(res$log_posterior_unnorm) ]
cat("Approx MAP (grid):", theta_map, "\n")

```

## Comparison of random number generators

In the previous exercise, we implemented a sampler for the posterior distribution of the parameter $\theta$ using an Rcpp function. An equivalent version of the same algorithm can also be implemented purely in R.

Although both implementations are based on the same statistical model and computational method, differences in numerical precision and implementation details may lead to small discrepancies in the generated random samples.

To assess whether the Rcpp implementation produces samples consistent with the original R implementation, we compare the generated random numbers using a **quantile–quantile plot (QQ plot)**.

## QQ plot methodology

Let $\{\theta^{(1)}_{\text{R}}, \dots, \theta^{(N)}_{\text{R}}\}$ denote the samples generated by the R function, and let $\{\theta^{(1)}_{\text{C++}}, \dots, \theta^{(N)}_{\text{C++}}\}$ denote the samples generated by the Rcpp function.

A QQ plot compares the empirical quantiles of these two samples. If both sets of random numbers follow the same distribution, the points in the QQ plot should lie approximately along the 45-degree reference line.

Systematic deviations from the diagonal would indicate differences between the two implementations, while small random fluctuations around the line are expected due to Monte Carlo variability.

Therefore, the QQ plot provides a graphical diagnostic for validating the correctness of the Rcpp implementation relative to the original R version.

```{r}
posterior_theta_grid_R <- function(counts,
                                   grid_n = 20001,
                                   n_samp = 5000,
                                   prior_a = 1,
                                   prior_b = 1,
                                   eps = 1e-12) {
  x1 <- counts[1]
  x2 <- counts[2]
  x3 <- counts[3]
  x4 <- counts[4]

  theta <- seq(eps, 1 - eps, length.out = grid_n)

  p1 <- 0.5 + theta / 4
  p2 <- (1 - theta) / 4
  p3 <- (1 - theta) / 4
  p4 <- theta / 4

  loglik <- x1 * log(p1) +
            x2 * log(p2) +
            x3 * log(p3) +
            x4 * log(p4)

  logprior <- (prior_a - 1) * log(theta) +
              (prior_b - 1) * log(1 - theta)

  logpost <- loglik + logprior
  w <- exp(logpost - max(logpost))
  w <- w / sum(w)

  sample(theta, size = n_samp, replace = TRUE, prob = w)
}

set.seed(123)

counts <- c(125L, 18L, 20L, 34L)

# R version
theta_R <- posterior_theta_grid_R(
  counts = counts,
  n_samp = 5000
)

# Rcpp version
theta_cpp <- posterior_theta_grid_cpp(
  counts = counts,
  n_samp = 5000
)$samples_theta

qqplot(theta_R, theta_cpp,
       xlab = "Quantiles of R-generated samples",
       ylab = "Quantiles of Rcpp-generated samples",
       main = "QQ plot: R vs Rcpp posterior samples of theta")

abline(0, 1, col = "red", lwd = 2)
```

## Comparison of computation time

In the previous exercise, two functions were implemented to generate random samples from the posterior distribution of the parameter $\theta$:

1. A pure R implementation based on grid approximation.
2. An Rcpp implementation using compiled C++ code.

Although both functions implement the same statistical algorithm and produce samples from the same target distribution, their computational performance may differ substantially due to differences in execution speed between interpreted R code and compiled C++ code.

To quantitatively compare the computational efficiency of the two implementations, we use the function `microbenchmark()`, which repeatedly evaluates expressions and records their execution times with high-resolution timing.

## Microbenchmark methodology

Let $T_{\text{R}}$ denote the execution time of the pure R function and $T_{\text{C++}}$ denote the execution time of the Rcpp function. The `microbenchmark()` function runs each implementation many times under identical conditions and summarizes the distribution of execution times.

By comparing summary statistics such as the median or mean execution time, we can assess the speedup achieved by using the Rcpp implementation. A substantially smaller execution time for the Rcpp function indicates the benefit of moving computationally intensive tasks from R to compiled C++ code.

```{r}
if (!requireNamespace("microbenchmark", quietly = TRUE)) {
  install.packages("microbenchmark")
}
library(microbenchmark)

counts <- c(125L, 18L, 20L, 34L)

grid_n <- 20001
n_samp <- 5000
bm <- microbenchmark(
  R_version = posterior_theta_grid_R(
    counts = counts,
    grid_n = grid_n,
    n_samp = n_samp
  ),
  Rcpp_version = posterior_theta_grid_cpp(
    counts = counts,
    grid_n = grid_n,
    n_samp = n_samp
  ),
  times = 50
)

print(bm)

boxplot(bm,
        main = "Computation time comparison: R vs Rcpp",
        ylab = "Execution time (nanoseconds)")

```




